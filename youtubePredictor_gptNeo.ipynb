{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "youtubePredictor_gptNeo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOij32HrNfYInfTw2sgm4KJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anthemwingate/jiNx/blob/main/youtubePredictor_gptNeo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAqX_F-QnBp9"
      },
      "source": [
        "**Initial Preparations**\n",
        "\n",
        "1.   Add Dependencies\n",
        "2.   Authenticate Google Cloud Storage\n",
        "3.   Create local directory and Clone GPTNeo repo from Github\n",
        "4.   Install requirements\n",
        "5.   Declare local variables\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dui9iqxAn2bf",
        "outputId": "63bd8fc6-789b-4254-f1d3-1fb94957f04d"
      },
      "source": [
        "#@title Dependencies\n",
        "!pip3 install --upgrade absl-py==0.11\n",
        "!pip3 install --upgrade tensorflow-metadata"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: absl-py==0.11 in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from absl-py==0.11) (1.15.0)\n",
            "Requirement already up-to-date: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (0.29.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf<4,>=3.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata) (3.15.6)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata) (1.53.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py<0.13,>=0.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata) (0.11.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.7->tensorflow-metadata) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZRKDg71nAni",
        "outputId": "4a96337d-8b14-4a07-cfbe-b81092c716c1"
      },
      "source": [
        "#@title Setup\n",
        "%tensorflow_version 2.x\n",
        "!git clone https://github.com/EleutherAI/GPTNeo\n",
        "%cd GPTNeo\n",
        "!pip3 install -q -r requirements.txt\n",
        "pretrained_model = None\n",
        "dataset = None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'GPTNeo' already exists and is not an empty directory.\n",
            "/content/GPTNeo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQmEaFm6nm-1",
        "outputId": "ba47e76e-fbe8-4037-c43d-171d1e3a5467"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!gcloud init"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Welcome! This command will take you through the configuration of gcloud.\n",
            "\n",
            "Settings from your current configuration [default] are:\n",
            "component_manager:\n",
            "  disable_update_check: 'True'\n",
            "compute:\n",
            "  gce_metadata_read_timeout_sec: '0'\n",
            "core:\n",
            "  account: anthemw@gmail.com\n",
            "\n",
            "Pick configuration to use:\n",
            " [1] Re-initialize this configuration [default] with new settings \n",
            " [2] Create a new configuration\n",
            "Please enter your numeric choice:  1\n",
            "\n",
            "Your current configuration has been set to: [default]\n",
            "\n",
            "You can skip diagnostics next time by using the following flag:\n",
            "  gcloud init --skip-diagnostics\n",
            "\n",
            "Network diagnostic detects and fixes local network connection issues.\n",
            "Reachability Check passed.\n",
            "Network diagnostic passed (1/1 checks passed).\n",
            "\n",
            "Choose the account you would like to use to perform operations for \n",
            "this configuration:\n",
            " [1] anthemw@gmail.com\n",
            " [2] Log in with a new account\n",
            "Please enter your numeric choice:  1\n",
            "\n",
            "You are logged in as: [anthemw@gmail.com].\n",
            "\n",
            "Pick cloud project to use: \n",
            " [1] single-patrol-308619\n",
            " [2] Create a new project\n",
            "Please enter numeric choice or text value (must exactly match list \n",
            "item):  1\n",
            "\n",
            "Your current project has been set to: [single-patrol-308619].\n",
            "\n",
            "Not setting default zone/region (this feature makes it easier to use\n",
            "[gcloud compute] by setting an appropriate default value for the\n",
            "--zone and --region flag).\n",
            "See https://cloud.google.com/compute/docs/gcloud-compute section on how to set\n",
            "default compute region and zone manually. If you would like [gcloud init] to be\n",
            "able to do this for you the next time you run it, make sure the\n",
            "Compute Engine API is enabled for your project on the\n",
            "https://console.developers.google.com/apis page.\n",
            "\n",
            "Your Google Cloud SDK is configured and ready to use!\n",
            "\n",
            "* Commands that require authentication will use anthemw@gmail.com by default\n",
            "* Commands will reference project `single-patrol-308619` by default\n",
            "Run `gcloud help config` to learn how to change individual settings\n",
            "\n",
            "This gcloud configuration is called [default]. You can create additional configurations if you work with multiple accounts and/or projects.\n",
            "Run `gcloud topic configurations` to learn more.\n",
            "\n",
            "Some things to try next:\n",
            "\n",
            "* Run `gcloud --help` to see the Cloud Platform services you can interact with. And run `gcloud help COMMAND` to get help on any gcloud command.\n",
            "* Run `gcloud topic --help` to learn about advanced features of the SDK like arg files and output formatting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwwTz-gxp48V"
      },
      "source": [
        "path_to_cloud_bucket = 'gs://ditto_gptneo/' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGt1MDOlqYaS"
      },
      "source": [
        "**Set Up Dataset**\n",
        "\n",
        "\n",
        "1.   Set Dataset YoutubeSubtitles and download data\n",
        "2.   set dataset path variable, data \n",
        "3.   Set dataset name ytsub variable, ytsubs\n",
        "4.   set out name variable, dataset name concatenated with tokenized\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByocrKFZqYFq",
        "outputId": "2485ff67-8250-41cf-e71a-11f9bcf9c83d"
      },
      "source": [
        "import os\n",
        "dataset = 'YoutubeSubtitles'\n",
        "os.makedirs('data', exist_ok=True)\n",
        "!wget https://the-eye.eu/public/AI/pile_preliminary_components/yt_subs.jsonl.zst -O data/yt_subs.jsonl.zst\n",
        "dataset_path = 'data'\n",
        "dataset_name = 'ytsubs'\n",
        "out_name = dataset_name + \"_tokenized\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-27 17:15:05--  https://the-eye.eu/public/AI/pile_preliminary_components/yt_subs.jsonl.zst\n",
            "Resolving the-eye.eu (the-eye.eu)... 162.213.130.242\n",
            "Connecting to the-eye.eu (the-eye.eu)|162.213.130.242|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1782264700 (1.7G) [application/octet-stream]\n",
            "Saving to: ‘data/yt_subs.jsonl.zst’\n",
            "\n",
            "data/yt_subs.jsonl. 100%[===================>]   1.66G   112MB/s    in 16s     \n",
            "\n",
            "2021-03-27 17:15:20 (108 MB/s) - ‘data/yt_subs.jsonl.zst’ saved [1782264700/1782264700]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoDqDeelrxKr"
      },
      "source": [
        "**Tokenize Dataset and Upload to Google Cloud Storage**\n",
        "\n",
        "\n",
        "1.   Create a file using dataset name variable, and write the dataset to that file in the dataset path variable\n",
        "2.   Copy dataset to Google Cloud Storage\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1OqH6HzrrIR",
        "outputId": "dc1880ca-35dd-40d2-b798-248fd3915568"
      },
      "source": [
        "# Tokenize Data\n",
        "!python data/create_tfrecords.py --input_dir /content/GPTNeo/$dataset_path --name $dataset_name --files_per 1000 --output_dir $out_name --write_dataset_config --processes 1\n",
        "\n",
        "# copy the data to your bucket\n",
        "if not path_to_cloud_bucket.endswith('/'):\n",
        "  path_to_cloud_bucket += '/'\n",
        "copy_loc = path_to_cloud_bucket + \"datasets/\" + dataset\n",
        "!gsutil -m cp -r /content/GPTNeo/$out_name $copy_loc\n",
        "!gsutil ls $path_to_cloud_bucket"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-27 17:19:36.102137: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Downloading: 100% 1.04M/1.04M [00:00<00:00, 5.82MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 3.26MB/s]\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 7.31MB/s]\n",
            "Writing TFRecord Files to ytsubs_tokenized/. Parsed 173619 input files. files_written : : 841it [2:44:19, 12.12s/it]{'discarded': 14935, 'processed': 173651, 'successful': 158716}\n",
            "Writing TFRecord Files to ytsubs_tokenized/. Parsed 173619 input files. files_written : : 841it [2:44:20, 11.73s/it]\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_152_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_385_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_215_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_259_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_267_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_730_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_554_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_103_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_480_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_382_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_426_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_335_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_464_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_381_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_588_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_369_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_374_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_665_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_760_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_207_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_591_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_803_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_828_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_664_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_534_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_285_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_202_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_370_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_193_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_788_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_144_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_536_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_543_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_830_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_298_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_122_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_758_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_747_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_406_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_729_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_546_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_343_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_254_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_653_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_490_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_598_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_222_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_355_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_130_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_507_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_628_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_433_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_388_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_769_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_501_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_469_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_623_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_456_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_487_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_92_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_225_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_539_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_142_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_295_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_1_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_17_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_840_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_643_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_170_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_210_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_445_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_154_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_617_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_192_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_482_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_807_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_325_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_630_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_138_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_45_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_805_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_13_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_87_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_20_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_814_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_108_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_502_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_372_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_809_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_698_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_373_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_657_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_151_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_515_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_28_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_841_464.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_723_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_413_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_567_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_442_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_529_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_91_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_119_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_100_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_427_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_330_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_358_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_301_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_68_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_790_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_376_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_793_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_218_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_23_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_838_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_416_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_512_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_250_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_637_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_115_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_711_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_823_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_605_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_804_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_245_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_404_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_692_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_726_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_687_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_342_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_189_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_475_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_488_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_0_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_509_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_727_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_706_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_60_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_656_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_361_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_544_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_260_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_10_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_346_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_384_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_143_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_720_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_839_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_348_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_319_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_522_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_268_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_504_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_300_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_196_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_818_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_625_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_800_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_2_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_602_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_738_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_577_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_37_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_314_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_199_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_253_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_345_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_3_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_326_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_724_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_786_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_77_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_435_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_334_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_751_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_835_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_538_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_238_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_421_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_252_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_124_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_700_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_762_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_631_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_660_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_597_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_570_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_109_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_443_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_424_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_495_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_367_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_204_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_560_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_455_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_391_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_309_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_749_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_387_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_432_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_228_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_721_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_691_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_781_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_555_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_791_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_194_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_98_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_291_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_547_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_4_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_575_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_537_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_827_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_27_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_31_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_650_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_431_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_673_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_820_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_561_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_541_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_184_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_308_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_742_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_197_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_824_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_772_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_763_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_434_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_461_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_523_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_141_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_663_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_837_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_322_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_438_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_181_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_33_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_167_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_668_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_641_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_390_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_693_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_209_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_158_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_639_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_755_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_297_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_494_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_757_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_18_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_671_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_741_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_145_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_624_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_418_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_403_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_449_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_6_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_777_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_120_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_670_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_472_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_636_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_683_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_165_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_125_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_54_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_685_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_392_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_610_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_81_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_457_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_611_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_530_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_485_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_707_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_640_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_183_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_169_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_632_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_498_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_168_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_731_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_178_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_836_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_217_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_26_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_274_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_275_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_672_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_715_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_545_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_619_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_783_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_396_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_96_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_527_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_513_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_186_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_622_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_149_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_532_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_289_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_819_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_49_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_264_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_785_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_486_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_616_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_22_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_652_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_180_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_414_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_127_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_481_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_645_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_292_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_155_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_675_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_216_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_105_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_425_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_306_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_287_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_231_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_519_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_324_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_429_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_402_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_826_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_411_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_557_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_448_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_172_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_277_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_55_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_592_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_104_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_626_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_331_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_525_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_686_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_364_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_246_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_603_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_496_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_323_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_288_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_111_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_517_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_329_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_833_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_493_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_508_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_200_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_817_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_752_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_135_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_226_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_579_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_83_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_147_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_333_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_397_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_690_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_303_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_662_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_582_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_131_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_166_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_205_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_160_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_678_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_126_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_63_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_601_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_822_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_595_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_134_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_162_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_489_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_676_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_344_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_409_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_36_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_531_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_290_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_32_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_580_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_146_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_71_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_766_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_733_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_564_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_86_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_241_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_661_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_377_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_524_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_506_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_188_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_29_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_802_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_410_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_646_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_753_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_235_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_97_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_35_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_511_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_704_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_73_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_318_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_101_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_520_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_514_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_615_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_214_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_467_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_11_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_684_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_473_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_689_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_175_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_7_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_810_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_19_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_594_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_15_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_798_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_72_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_553_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_703_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_219_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_441_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_516_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_795_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_24_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_618_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_483_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_85_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_780_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_64_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_78_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_572_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_834_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_182_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_283_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_223_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_746_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_775_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_185_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_393_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_789_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_688_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_69_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_459_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_533_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_647_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_269_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_224_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_317_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_139_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_776_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_768_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_677_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_282_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_336_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_296_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_585_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_702_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_654_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_380_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_386_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_593_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_492_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_212_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_359_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_107_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_710_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_774_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_606_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_258_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_612_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_552_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_651_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_737_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_117_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_732_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_463_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_310_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_247_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_171_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_633_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_102_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_422_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_709_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_696_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_118_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_328_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_62_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_407_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_576_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_452_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_796_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_713_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_528_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_79_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_234_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_8_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_680_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_400_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_535_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_821_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_272_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_474_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_607_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_378_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_249_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_771_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_206_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_240_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_735_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_164_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_211_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_43_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_191_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_430_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_756_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_565_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_113_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_294_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_82_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_401_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_279_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_761_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_722_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_88_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_273_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_465_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_437_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_313_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_398_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_699_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_25_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_571_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_99_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_34_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_190_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_581_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_497_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_574_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_80_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_604_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_479_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_562_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_51_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_161_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_568_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_453_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_52_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_360_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_95_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_573_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_42_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_366_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_229_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_121_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_714_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/checkpoint.txt [Content-Type=text/plain]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_286_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_813_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_694_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_47_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_736_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_293_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_53_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_159_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_559_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_251_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_518_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_712_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_609_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_74_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_220_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_806_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_383_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_815_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_356_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_681_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_642_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_792_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_338_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_596_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_499_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_563_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_239_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_770_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_627_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_304_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_44_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_832_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_708_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_613_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_831_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_243_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_110_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_695_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_471_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_16_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_521_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_759_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_203_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_132_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_244_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_439_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_666_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_299_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_75_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_408_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_679_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_332_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_213_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_394_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_70_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_351_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_354_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_59_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_379_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_716_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_341_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_30_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_375_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_320_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_21_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_584_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_460_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_321_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_697_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_362_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_578_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_280_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_76_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_797_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_257_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_198_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_444_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_140_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_635_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_734_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_450_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_176_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_799_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_233_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_614_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_67_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_655_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_784_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_195_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_5_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_658_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_208_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_503_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_38_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_634_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_748_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_106_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_500_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_179_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_41_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_262_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_600_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_123_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_458_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_271_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_232_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_417_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_153_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_136_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_556_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_276_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_163_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_682_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_46_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_112_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_116_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_9_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_39_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_114_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_201_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_491_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_750_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_542_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_173_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_644_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_412_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_754_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_363_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_66_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_548_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_352_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_261_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_649_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_227_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_174_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_569_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_446_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_739_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_90_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_566_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_440_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_719_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_674_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_505_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_57_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_718_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_128_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_405_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_767_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_93_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_773_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_587_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_368_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_419_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_340_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_237_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_669_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_551_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_420_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_230_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_270_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_468_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_365_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_744_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_510_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_782_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_61_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_14_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_40_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_476_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_447_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_148_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_801_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_177_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_157_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_794_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_550_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_629_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_187_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_58_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_436_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_586_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_765_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_477_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_129_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_353_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_454_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_327_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_811_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_307_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_787_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_265_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_740_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_248_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_278_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_745_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_778_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_621_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_589_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_371_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_590_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_779_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_316_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_484_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_829_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_717_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_648_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_725_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_808_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_583_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_659_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_357_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_478_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_302_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_428_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_305_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_638_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_312_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_764_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_415_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_825_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_349_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_150_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_94_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_133_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_816_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_743_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_451_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_728_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_339_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_56_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_558_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_311_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_84_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_608_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_337_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_12_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_89_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_281_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_284_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_242_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_65_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_466_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_221_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_620_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_540_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_350_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_599_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_462_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_705_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_50_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_256_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_526_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_48_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_263_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_137_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_667_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_395_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_701_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_347_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_812_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_236_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_399_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_255_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_389_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_266_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_549_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_423_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_315_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_470_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/ytsubs_tokenized/ytsubs_156_0_1000.tfrecords [Content-Type=application/octet-stream]...\n",
            "\\\n",
            "Operation completed over 843 objects/3.2 GiB.                                    \n",
            "gs://ditto_gptneo/datasets/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAtFICb0syg8"
      },
      "source": [
        "**Set Model Configs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIl0xQqusyyJ",
        "outputId": "e888a845-51d2-41f2-8aea-238ff892b5f5"
      },
      "source": [
        "%%writefile configs/dataset_configs/ytsubs.json\n",
        "\n",
        "{\n",
        "  \"path\": \"gs://ditto_gptneo/datasets/dataset_configs/ytsubs*.tfrecords\",\n",
        "  \"eval_path\": \"\",\n",
        "  \"n_vocab\": 50256,\n",
        "  \"tokenizer_is_pretrained\": true,\n",
        "  \"tokenizer_path\": \"gpt2\",\n",
        "  \"eos_id\": 50256,\n",
        "  \"padding_id\": 50257\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing configs/dataset_configs/ytsubs.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUC6cO-6ukif",
        "outputId": "821f2044-344e-4277-f44a-9b5db3ed537c"
      },
      "source": [
        "%%writefile configs/GPT3_XL.json\n",
        "\n",
        "{\n",
        "    \"n_head\": 16,\n",
        "    \"n_vocab\": 50257,\n",
        "    \"embed_dropout\": 0,\n",
        "    \"lr\": 0.0002,\n",
        "    \"lr_decay\": \"cosine\",\n",
        "    \"warmup_steps\": 3000,\n",
        "    \"beta1\": 0.9,\n",
        "    \"beta2\": 0.95,\n",
        "    \"epsilon\": 1e-8,\n",
        "    \"opt_name\": \"adam\",\n",
        "    \"weight_decay\": 0,\n",
        "    \"train_batch_size\": 256,\n",
        "    \"attn_dropout\": 0,\n",
        "    \"train_steps\": 600000,\n",
        "    \"eval_steps\": 0,\n",
        "    \"predict_steps\": 1,\n",
        "    \"res_dropout\": 0,\n",
        "    \"eval_batch_size\": 4,\n",
        "    \"predict_batch_size\": 1,\n",
        "    \"iterations\": 100,\n",
        "    \"n_embd\": 2048,\n",
        "    \"datasets\": [[\"pile\", null, null, null]],\n",
        "    \"model\": \"GPT\",\n",
        "    \"model_path\": \"gs://ditto_gptneo/GPT3_XL\",\n",
        "    \"n_ctx\": 2048,\n",
        "    \"n_layer\": 24,\n",
        "    \"scale_by_depth\": true,\n",
        "    \"scale_by_in\": false,\n",
        "    \"attention_types\" :  [[[\"global\", \"local\"],12]],\n",
        "    \"mesh_shape\": \"x:4,y:2\",\n",
        "    \"layout\": \"intermediate_expanded:x,heads:x,vocab:n_vocab,memory_length:y,embd:y\",\n",
        "    \"activation_function\": \"gelu\",\n",
        "    \"recompute_grad\": true,\n",
        "    \"gradient_clipping\": 1.0,\n",
        "    \"tokens_per_mb_per_replica\": 2048,\n",
        "    \"precision\": \"bfloat16\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing configs/GPT3_XL.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMdk-5KpvJMz"
      },
      "source": [
        "**Pretrained Model**\n",
        "\n",
        "\n",
        "1.   Download the pretrained model weights\n",
        "2.   Set path to local weight variable\n",
        "3.   Upload the model to Google Cloud Storeage\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I9BFG4yvJaJ",
        "outputId": "10fac8ea-35ab-4012-b3f9-b8e861169102"
      },
      "source": [
        "# @title Download pretrained model weights:\n",
        "pretrained_model = 'GPT3_XL' #@param [\"GPT3_XL\", \"GPT3_1-3B\"]\n",
        "!wget -m -np -c -U \"eye02\" -w 2 -R \"index.html*\" \"https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/\"\n",
        "path_to_local_weights = f\"/content/GPTNeo/the-eye.eu/public/AI/gptneo-release/{pretrained_model}\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-27 20:08:28--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/\n",
            "Resolving the-eye.eu (the-eye.eu)... 162.213.130.242\n",
            "Connecting to the-eye.eu (the-eye.eu)|162.213.130.242|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/index.html.tmp’\n",
            "\n",
            "\r          the-eye.e     [<=>                 ]       0  --.-KB/s               \rthe-eye.eu/public/A     [ <=>                ]  10.35K  --.-KB/s    in 0s      \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2021-03-27 20:08:29 (77.6 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/index.html.tmp’ saved [10598]\n",
            "\n",
            "Loading robots.txt; please ignore errors.\n",
            "--2021-03-27 20:08:31--  https://the-eye.eu/robots.txt\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4 [text/plain]\n",
            "Saving to: ‘the-eye.eu/robots.txt’\n",
            "\n",
            "the-eye.eu/robots.t 100%[===================>]       4  --.-KB/s    in 0s      \n",
            "\n",
            "2021-03-27 20:08:31 (1.27 MB/s) - ‘the-eye.eu/robots.txt’ saved [4/4]\n",
            "\n",
            "Removing the-eye.eu/public/AI/gptneo-release/GPT3_XL/index.html.tmp since it should be rejected.\n",
            "\n",
            "--2021-03-27 20:08:33--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/checkpoint\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 523 [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/checkpoint’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>]     523  --.-KB/s    in 0s      \n",
            "\n",
            "2021-03-27 20:08:33 (167 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/checkpoint’ saved [523/523]\n",
            "\n",
            "--2021-03-27 20:08:35--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/config.json\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 934 [application/json]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/config.json’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>]     934  --.-KB/s    in 0s      \n",
            "\n",
            "2021-03-27 20:08:35 (249 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/config.json’ saved [934/934]\n",
            "\n",
            "--2021-03-27 20:08:37--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00000-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8 [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00000-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>]       8  --.-KB/s    in 0s      \n",
            "\n",
            "2021-03-27 20:08:37 (697 KB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00000-of-00032’ saved [8/8]\n",
            "\n",
            "--2021-03-27 20:08:39--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00001-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 427995136 (408M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00001-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 408.17M   110MB/s    in 3.9s    \n",
            "\n",
            "2021-03-27 20:08:43 (105 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00001-of-00032’ saved [427995136/427995136]\n",
            "\n",
            "--2021-03-27 20:08:45--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00002-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 457625600 (436M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00002-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 436.43M   113MB/s    in 4.0s    \n",
            "\n",
            "2021-03-27 20:08:49 (108 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00002-of-00032’ saved [457625600/457625600]\n",
            "\n",
            "--2021-03-27 20:08:51--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00003-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 394539008 (376M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00003-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 376.26M   107MB/s    in 3.6s    \n",
            "\n",
            "2021-03-27 20:08:55 (103 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00003-of-00032’ saved [394539008/394539008]\n",
            "\n",
            "--2021-03-27 20:08:57--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00004-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 394477568 (376M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00004-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 376.20M   106MB/s    in 3.7s    \n",
            "\n",
            "2021-03-27 20:09:01 (101 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00004-of-00032’ saved [394477568/394477568]\n",
            "\n",
            "--2021-03-27 20:09:03--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00005-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 461422592 (440M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00005-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 440.05M   112MB/s    in 4.1s    \n",
            "\n",
            "2021-03-27 20:09:07 (107 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00005-of-00032’ saved [461422592/461422592]\n",
            "\n",
            "--2021-03-27 20:09:09--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00006-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 402784256 (384M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00006-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 384.12M   111MB/s    in 3.6s    \n",
            "\n",
            "2021-03-27 20:09:12 (107 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00006-of-00032’ saved [402784256/402784256]\n",
            "\n",
            "--2021-03-27 20:09:14--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00007-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 402849792 (384M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00007-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 384.19M  92.4MB/s    in 4.8s    \n",
            "\n",
            "2021-03-27 20:09:19 (80.1 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00007-of-00032’ saved [402849792/402849792]\n",
            "\n",
            "--2021-03-27 20:09:21--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00008-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 478871552 (457M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00008-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 456.69M  80.4MB/s    in 5.4s    \n",
            "\n",
            "2021-03-27 20:09:27 (84.8 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00008-of-00032’ saved [478871552/478871552]\n",
            "\n",
            "--2021-03-27 20:09:29--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00009-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 419491840 (400M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00009-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 400.06M  88.5MB/s    in 4.7s    \n",
            "\n",
            "2021-03-27 20:09:33 (85.7 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00009-of-00032’ saved [419491840/419491840]\n",
            "\n",
            "--2021-03-27 20:09:35--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00010-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 453316608 (432M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00010-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 432.32M  60.8MB/s    in 6.4s    \n",
            "\n",
            "2021-03-27 20:09:42 (67.5 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00010-of-00032’ saved [453316608/453316608]\n",
            "\n",
            "--2021-03-27 20:09:44--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00011-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 402812928 (384M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00011-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 384.15M  65.9MB/s    in 6.0s    \n",
            "\n",
            "2021-03-27 20:09:50 (64.0 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00011-of-00032’ saved [402812928/402812928]\n",
            "\n",
            "--2021-03-27 20:09:52--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00012-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 394379264 (376M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00012-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 376.11M  69.4MB/s    in 5.6s    \n",
            "\n",
            "2021-03-27 20:09:58 (66.8 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00012-of-00032’ saved [394379264/394379264]\n",
            "\n",
            "--2021-03-27 20:10:00--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00013-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 428032000 (408M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00013-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 408.20M  87.0MB/s    in 5.1s    \n",
            "\n",
            "2021-03-27 20:10:05 (80.4 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00013-of-00032’ saved [428032000/428032000]\n",
            "\n",
            "--2021-03-27 20:10:07--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00014-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 419528704 (400M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00014-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 400.09M   110MB/s    in 3.8s    \n",
            "\n",
            "2021-03-27 20:10:11 (104 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00014-of-00032’ saved [419528704/419528704]\n",
            "\n",
            "--2021-03-27 20:10:13--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00015-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 419524608 (400M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00015-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 400.09M  79.2MB/s    in 4.7s    \n",
            "\n",
            "2021-03-27 20:10:17 (84.7 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00015-of-00032’ saved [419524608/419524608]\n",
            "\n",
            "--2021-03-27 20:10:19--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00016-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 444727296 (424M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00016-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 424.12M  70.0MB/s    in 6.1s    \n",
            "\n",
            "2021-03-27 20:10:26 (69.4 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00016-of-00032’ saved [444727296/444727296]\n",
            "\n",
            "--2021-03-27 20:10:28--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00017-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 453287936 (432M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00017-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 432.29M  73.7MB/s    in 6.0s    \n",
            "\n",
            "2021-03-27 20:10:34 (72.2 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00017-of-00032’ saved [453287936/453287936]\n",
            "\n",
            "--2021-03-27 20:10:36--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00018-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 444751872 (424M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00018-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 424.15M  80.3MB/s    in 5.6s    \n",
            "\n",
            "2021-03-27 20:10:41 (76.3 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00018-of-00032’ saved [444751872/444751872]\n",
            "\n",
            "--2021-03-27 20:10:43--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00019-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 402776064 (384M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00019-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 384.12M  89.5MB/s    in 4.4s    \n",
            "\n",
            "2021-03-27 20:10:48 (86.8 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00019-of-00032’ saved [402776064/402776064]\n",
            "\n",
            "--2021-03-27 20:10:50--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00020-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 402817024 (384M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00020-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 384.16M  95.9MB/s    in 4.2s    \n",
            "\n",
            "2021-03-27 20:10:54 (91.1 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00020-of-00032’ saved [402817024/402817024]\n",
            "\n",
            "--2021-03-27 20:10:56--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00021-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 427991040 (408M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00021-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 408.16M   105MB/s    in 4.0s    \n",
            "\n",
            "2021-03-27 20:11:00 (101 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00021-of-00032’ saved [427991040/427991040]\n",
            "\n",
            "--2021-03-27 20:11:02--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00022-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 444710912 (424M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00022-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 424.11M  86.9MB/s    in 4.7s    \n",
            "\n",
            "2021-03-27 20:11:07 (90.2 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00022-of-00032’ saved [444710912/444710912]\n",
            "\n",
            "--2021-03-27 20:11:09--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00023-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 402776064 (384M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00023-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 384.12M  98.2MB/s    in 4.1s    \n",
            "\n",
            "2021-03-27 20:11:13 (94.2 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00023-of-00032’ saved [402776064/402776064]\n",
            "\n",
            "--2021-03-27 20:11:15--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00024-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 402862080 (384M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00024-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 384.20M  80.0MB/s    in 4.7s    \n",
            "\n",
            "2021-03-27 20:11:20 (82.2 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00024-of-00032’ saved [402862080/402862080]\n",
            "\n",
            "--2021-03-27 20:11:22--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00025-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 428011520 (408M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00025-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 408.18M  86.9MB/s    in 4.8s    \n",
            "\n",
            "2021-03-27 20:11:27 (84.5 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00025-of-00032’ saved [428011520/428011520]\n",
            "\n",
            "--2021-03-27 20:11:29--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00026-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 411336704 (392M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00026-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 392.28M  48.3MB/s    in 8.4s    \n",
            "\n",
            "2021-03-27 20:11:37 (46.6 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00026-of-00032’ saved [411336704/411336704]\n",
            "\n",
            "--2021-03-27 20:11:39--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00027-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 402817024 (384M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00027-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 384.16M  47.7MB/s    in 7.3s    \n",
            "\n",
            "2021-03-27 20:11:47 (52.5 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00027-of-00032’ saved [402817024/402817024]\n",
            "\n",
            "--2021-03-27 20:11:49--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00028-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 402849792 (384M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00028-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 384.19M  46.0MB/s    in 8.7s    \n",
            "\n",
            "2021-03-27 20:11:57 (44.2 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00028-of-00032’ saved [402849792/402849792]\n",
            "\n",
            "--2021-03-27 20:11:59--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00029-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 444715008 (424M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00029-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 424.11M  62.2MB/s    in 7.7s    \n",
            "\n",
            "2021-03-27 20:12:07 (55.1 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00029-of-00032’ saved [444715008/444715008]\n",
            "\n",
            "--2021-03-27 20:12:09--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00030-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 478871552 (457M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00030-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 456.69M   114MB/s    in 4.2s    \n",
            "\n",
            "2021-03-27 20:12:13 (110 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00030-of-00032’ saved [478871552/478871552]\n",
            "\n",
            "--2021-03-27 20:12:15--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00031-of-00032\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 402804736 (384M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00031-of-00032’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 384.14M   112MB/s    in 3.6s    \n",
            "\n",
            "2021-03-27 20:12:19 (106 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00031-of-00032’ saved [402804736/402804736]\n",
            "\n",
            "--2021-03-27 20:12:21--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.index\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15417 (15K) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.index’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>]  15.06K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-03-27 20:12:21 (92.1 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.index’ saved [15417/15417]\n",
            "\n",
            "--2021-03-27 20:12:23--  https://the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.meta\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27851302 (27M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.meta’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>]  26.56M  71.2MB/s    in 0.4s    \n",
            "\n",
            "2021-03-27 20:12:23 (71.2 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.meta’ saved [27851302/27851302]\n",
            "\n",
            "FINISHED --2021-03-27 20:12:23--\n",
            "Total wall clock time: 3m 55s\n",
            "Downloaded: 38 files, 12G in 2m 38s (79.4 MB/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPb2xFT1wQv_",
        "outputId": "aab6c98d-4760-4a83-f8d1-708ccdcefeb3"
      },
      "source": [
        "# upload to your bucket\n",
        "bucket_base = \"gs://\" + path_to_cloud_bucket.replace('gs://', '').split('/')[0]\n",
        "!gsutil -m cp -r $path_to_local_weights $bucket_base"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00011-of-00032 [Content-Type=application/octet-stream]...\n",
            "/ [0/36 files][    0.0 B/ 12.3 GiB]   0% Done                                   \rCopying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00014-of-00032 [Content-Type=application/octet-stream]...\n",
            "/ [0/36 files][    0.0 B/ 12.3 GiB]   0% Done                                   \rCopying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00023-of-00032 [Content-Type=application/octet-stream]...\n",
            "/ [0/36 files][    0.0 B/ 12.3 GiB]   0% Done                                   \r==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00022-of-00032 [Content-Type=application/octet-stream]...\n",
            "/ [0/36 files][    0.0 B/ 12.3 GiB]   0% Done                                   \rCopying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00016-of-00032 [Content-Type=application/octet-stream]...\n",
            "/ [0/36 files][    0.0 B/ 12.3 GiB]   0% Done                                   \rCopying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.index [Content-Type=application/octet-stream]...\n",
            "/ [0/36 files][    0.0 B/ 12.3 GiB]   0% Done                                   \rCopying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.meta [Content-Type=application/octet-stream]...\n",
            "/ [0/36 files][    0.0 B/ 12.3 GiB]   0% Done                                   \rCopying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00030-of-00032 [Content-Type=application/octet-stream]...\n",
            "/ [0/36 files][    0.0 B/ 12.3 GiB]   0% Done                                   \rCopying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00017-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/checkpoint [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00015-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00028-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00020-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00004-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00003-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00024-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00019-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00027-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00009-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00012-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00025-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00018-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00002-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00006-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00005-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00031-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00021-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00008-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00010-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00001-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/config.json [Content-Type=application/json]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00013-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00007-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00000-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00029-of-00032 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/GPTNeo/the-eye.eu/public/AI/gptneo-release/GPT3_XL/model.ckpt-362000.data-00026-of-00032 [Content-Type=application/octet-stream]...\n",
            "| [36/36 files][ 12.3 GiB/ 12.3 GiB] 100% Done  50.2 MiB/s ETA 00:00:00         \n",
            "Operation completed over 36 objects/12.3 GiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L2CiDDhwpyb",
        "outputId": "ec9abed8-4996-4184-d0ef-2364deaf0f00"
      },
      "source": [
        "!gsutil ls $bucket_base"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://ditto_gptneo/GPT3_XL/\n",
            "gs://ditto_gptneo/datasets/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRVbsKNsw0pd",
        "outputId": "ec17cf59-5063-4840-e9f9-c3ff7eedbf7a"
      },
      "source": [
        "# @title Modify config for colab. \n",
        "  \n",
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "path_to_model = \"gs://ditto_gptneo/GPT3_XL\" #@param {type:\"string\"}\n",
        "batch_size = 8 #@param {type:\"integer\"}\n",
        "dset = \"ytsubs\"  #@param {type:\"string\"}\n",
        "mesh_shape = \"x:4,y:2\" #@param {type:\"string\"}\n",
        "train_steps = 1000 #@param {type:\"integer\"}\n",
        "steps_per_checkpoint = 500 #@param {type:\"integer\"}\n",
        "start_step = 400000 if pretrained_model == \"GPT3_2-7B\" else 362000\n",
        "\n",
        "if path_to_model == \"\":\n",
        "  path_to_model = f'{bucket_base.strip(\"/\")}/{pretrained_model}'\n",
        "print(f'MODEL PATH: {path_to_model}\\n')\n",
        "\n",
        "if dset == \"\" and dataset != \"Sampling_Only\":\n",
        "  dset = dataset\n",
        "elif dataset is None:\n",
        "  dset = \"pile\"\n",
        "\n",
        "def pad_to_multiple_of(n, mult):\n",
        "  \"\"\"\n",
        "  pads n to a multiple of mult\n",
        "  \"\"\"\n",
        "  extra = n % mult\n",
        "  if extra > 0:\n",
        "      n = n + mult - extra\n",
        "  return n\n",
        "\n",
        "with open(f'{path_to_local_weights}/config.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "  pprint(data)\n",
        "  dset_val = [[dset, None, None, None]] if dset != \"\" else data[\"datasets\"]\n",
        "  mods = {\n",
        "          \"mesh_shape\": mesh_shape,\n",
        "          \"layout\": \"intermediate_expanded:x,heads:x,memory_length:y,embd:y\",\n",
        "          \"model_path\": path_to_model,\n",
        "          \"datasets\": dset_val,\n",
        "          \"train_steps\": start_step + train_steps,\n",
        "          \"eval_steps\": 0,\n",
        "          \"train_batch_size\": batch_size,\n",
        "          \"predict_batch_size\": batch_size\n",
        "        }\n",
        "  data.update(mods)\n",
        "  print('\\n--->\\n')\n",
        "  pprint(data)\n",
        "  with open(f'configs/{pretrained_model}.json', 'w') as outfile:\n",
        "    json.dump(data, outfile, indent=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MODEL PATH: gs://ditto_gptneo/GPT3_XL\n",
            "\n",
            "{'activation_function': 'gelu',\n",
            " 'attention_types': [[['global', 'local'], 12]],\n",
            " 'attn_dropout': 0,\n",
            " 'beta1': 0.9,\n",
            " 'beta2': 0.95,\n",
            " 'datasets': [['pile', None, None, None]],\n",
            " 'embed_dropout': 0,\n",
            " 'eos_id': 50256,\n",
            " 'epsilon': 1e-08,\n",
            " 'eval_batch_size': 128,\n",
            " 'eval_steps': 10,\n",
            " 'gradient_clipping': 1.0,\n",
            " 'iterations': 500,\n",
            " 'layout': 'batch:x,memory_length:y,embd:y',\n",
            " 'lr': 0.0002,\n",
            " 'lr_decay': 'cosine',\n",
            " 'lr_decay_end': 300000,\n",
            " 'mesh_shape': 'x:128,y:2',\n",
            " 'model_path': 'gs://neo-d/models/GPT3_XL_Pile',\n",
            " 'n_ctx': 2048,\n",
            " 'n_embd': 2048,\n",
            " 'n_head': 16,\n",
            " 'n_layer': 24,\n",
            " 'n_vocab': 50257,\n",
            " 'opt_name': 'adam',\n",
            " 'padding_id': 50257,\n",
            " 'precision': 'bfloat16',\n",
            " 'predict_batch_size': 128,\n",
            " 'predict_steps': 0,\n",
            " 'recompute_grad': True,\n",
            " 'res_dropout': 0,\n",
            " 'scale_by_depth': True,\n",
            " 'scale_by_in': False,\n",
            " 'tokens_per_mb_per_replica': 4096,\n",
            " 'train_batch_size': 512,\n",
            " 'train_steps': 400000,\n",
            " 'warmup_steps': 3000,\n",
            " 'weight_decay': 0}\n",
            "\n",
            "--->\n",
            "\n",
            "{'activation_function': 'gelu',\n",
            " 'attention_types': [[['global', 'local'], 12]],\n",
            " 'attn_dropout': 0,\n",
            " 'beta1': 0.9,\n",
            " 'beta2': 0.95,\n",
            " 'datasets': [['ytsubs', None, None, None]],\n",
            " 'embed_dropout': 0,\n",
            " 'eos_id': 50256,\n",
            " 'epsilon': 1e-08,\n",
            " 'eval_batch_size': 128,\n",
            " 'eval_steps': 0,\n",
            " 'gradient_clipping': 1.0,\n",
            " 'iterations': 500,\n",
            " 'layout': 'intermediate_expanded:x,heads:x,memory_length:y,embd:y',\n",
            " 'lr': 0.0002,\n",
            " 'lr_decay': 'cosine',\n",
            " 'lr_decay_end': 300000,\n",
            " 'mesh_shape': 'x:4,y:2',\n",
            " 'model_path': 'gs://ditto_gptneo/GPT3_XL',\n",
            " 'n_ctx': 2048,\n",
            " 'n_embd': 2048,\n",
            " 'n_head': 16,\n",
            " 'n_layer': 24,\n",
            " 'n_vocab': 50257,\n",
            " 'opt_name': 'adam',\n",
            " 'padding_id': 50257,\n",
            " 'precision': 'bfloat16',\n",
            " 'predict_batch_size': 8,\n",
            " 'predict_steps': 0,\n",
            " 'recompute_grad': True,\n",
            " 'res_dropout': 0,\n",
            " 'scale_by_depth': True,\n",
            " 'scale_by_in': False,\n",
            " 'tokens_per_mb_per_replica': 4096,\n",
            " 'train_batch_size': 8,\n",
            " 'train_steps': 363000,\n",
            " 'warmup_steps': 3000,\n",
            " 'weight_decay': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U3i9eUwxZeC"
      },
      "source": [
        "**Sample from the model**\n",
        "\n",
        "\n",
        "1.   Add prompt file to local colab directory using colab file upload functionality in browser menu\n",
        "2.   Execute Model Sampling\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_Fce-6fxYeD"
      },
      "source": [
        "#@title Add Prompt DiTTO_prompt and Execute"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWdEhceCxs54",
        "outputId": "70d8dcc2-3c24-4ca8-a75b-19c23af94fb8"
      },
      "source": [
        "%%writefile DiTTo_prompt.txt\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # input embedding stem\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "        # transformer\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "        # decoder head\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        self.block_size = config.block_size\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing DiTTo_prompt.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eDd2TpA06UB",
        "outputId": "8b14c087-02be-48ae-b5eb-f41bc58f5ccf"
      },
      "source": [
        "!python3 main.py --model $pretrained_model --steps_per_checkpoint 500 --tpu colab --predict --prompt DiTTo_prompt.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-27 20:29:46.607048: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "Current step 362000\n",
            "Saving config to gs://ditto_gptneo/GPT3_XL\n",
            "2021-03-27 20:30:04.934964: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-03-27 20:30:04.936865: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-03-27 20:30:04.950177: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-03-27 20:30:04.950235: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (b9f32c1e6fe8): /proc/driver/nvidia/version does not exist\n",
            "2021-03-27 20:30:06.110147: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
            "Done!\n",
            "params = defaultdict(<function fetch_model_params.<locals>.<lambda> at 0x7f0284bccf80>, {'n_head': 16, 'n_vocab': 50257, 'embed_dropout': 0, 'lr': 0.0002, 'lr_decay': 'cosine', 'warmup_steps': 3000, 'beta1': 0.9, 'beta2': 0.95, 'epsilon': 1e-08, 'opt_name': 'adam', 'weight_decay': 0, 'train_batch_size': 8, 'attn_dropout': 0, 'train_steps': 363000, 'lr_decay_end': 300000, 'eval_steps': 0, 'predict_steps': 0, 'res_dropout': 0, 'eval_batch_size': 128, 'predict_batch_size': 8, 'iterations': 500, 'n_embd': 2048, 'datasets': [['ytsubs', None, None, None]], 'model_path': 'gs://ditto_gptneo/GPT3_XL', 'n_ctx': 2048, 'n_layer': 24, 'scale_by_depth': True, 'scale_by_in': False, 'attention_types': ['global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local'], 'mesh_shape': 'x:4,y:2', 'layout': 'intermediate_expanded:x,heads:x,memory_length:y,embd:y', 'activation_function': 'gelu', 'recompute_grad': True, 'gradient_clipping': 1.0, 'tokens_per_mb_per_replica': 4096, 'precision': 'bfloat16', 'padding_id': 50257, 'eos_id': 50256, 'dataset_configs': {'ytsubs': {'path': 'gs://ditto_gptneo/datasets/dataset_configs/ytsubs*.tfrecords', 'eval_path': '', 'n_vocab': 50256, 'tokenizer_is_pretrained': True, 'tokenizer_path': 'gpt2', 'eos_id': 50256, 'padding_id': 50257}}, 'mlm_training': False, 'causal': True, 'num_cores': 8, 'auto_layout': False, 'auto_layout_and_mesh_shape': False, 'use_tpu': True, 'gpu_ids': ['device:GPU:0'], 'steps_per_checkpoint': 500, 'predict': True, 'model': 'GPT', 'export': False, 'sampling_use_entmax': False, 'moe_layers': None, 'slow_sampling': False})\n",
            "Using config: {'_model_dir': 'gs://ditto_gptneo/GPT3_XL', '_tf_random_seed': None, '_save_summary_steps': 500, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.40.85.146:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.40.85.146:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.40.85.146:8470', '_evaluation_master': 'grpc://10.40.85.146:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=500, num_shards=8, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7f0284c4b250>}\n",
            "_TPUContext: eval_on_tpu True\n",
            "Predictions generated\n",
            "Querying Tensorflow master (grpc://10.40.85.146:8470) for TPU system metadata.\n",
            "2021-03-27 20:30:07.020672: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "Initializing TPU system (master: grpc://10.40.85.146:8470) to fetch topology for model parallelism. This might take a while.\n",
            "Found TPU system:\n",
            "*** Num TPU Cores: 8\n",
            "*** Num TPU Workers: 1\n",
            "*** Num TPU Cores Per Worker: 8\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 6249188509115223616)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -4146733154084846229)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 5892924184187693777)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 2462857471687540533)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5469279350878100045)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, -9117572227669529228)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -3216840776060473709)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, -9064804662247500787)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -2631494962670085392)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 1762238546668444786)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 1041094745256039506)\n",
            "Calling model_fn.\n",
            "num_cores_per_replica: 1\n",
            "computation_shape: [1, 1, 1, 1]\n",
            "num_replicas: 8\n",
            "device_assignment.topology.device_coordinates: [[[0 0 0 0]\n",
            "  [0 0 0 1]\n",
            "  [1 0 0 0]\n",
            "  [1 0 0 1]\n",
            "  [0 1 0 0]\n",
            "  [0 1 0 1]\n",
            "  [1 1 0 0]\n",
            "  [1 1 0 1]]]\n",
            "device_assignment.core_assignment: [[[0 0 0 0]]\n",
            "\n",
            " [[0 0 0 1]]\n",
            "\n",
            " [[1 0 0 0]]\n",
            "\n",
            " [[1 0 0 1]]\n",
            "\n",
            " [[0 1 0 0]]\n",
            "\n",
            " [[0 1 0 1]]\n",
            "\n",
            " [[1 1 0 0]]\n",
            "\n",
            " [[1 1 0 1]]]\n",
            "2021-03-27 20:30:19.466558: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "device_list = ['/job:worker/task:0/device:CPU:0']\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "SimdMeshImpl init: Shape[x=4, y=2] LayoutRules{('intermediate_expanded', 'x'), ('embd', 'y'), ('memory_length', 'y'), ('heads', 'x')}\n",
            "Device Assignment: <tensorflow.python.tpu.device_assignment.DeviceAssignment object at 0x7f027f399f10>\n",
            "Create pnum_tensor\n",
            "Variable gpt2/h0/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h0/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h0/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h0/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h0/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h0/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h1/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h1/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h1/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h1/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h1/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h1/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h10/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h10/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h10/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h10/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h10/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h10/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h11/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h11/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h11/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h11/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h11/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h11/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h12/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h12/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h12/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h12/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h12/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h12/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h13/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h13/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h13/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h13/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h13/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h13/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h14/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h14/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h14/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h14/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h14/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h14/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h15/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h15/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h15/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h15/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h15/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h15/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h16/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h16/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h16/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h16/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h16/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h16/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h17/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h17/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h17/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h17/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h17/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h17/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h18/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h18/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h18/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h18/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h18/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h18/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h19/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h19/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h19/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h19/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h19/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h19/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h2/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h2/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h2/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h2/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h2/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h2/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h20/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h20/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h20/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h20/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h20/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h20/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h21/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h21/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h21/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h21/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h21/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h21/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h22/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h22/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h22/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h22/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h22/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h22/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h23/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h23/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h23/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h23/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h23/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h23/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h3/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h3/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h3/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h3/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h3/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h3/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h4/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h4/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h4/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h4/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h4/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h4/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h5/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h5/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h5/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h5/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h5/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h5/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h6/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h6/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h6/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h6/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h6/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h6/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h7/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h7/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h7/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h7/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h7/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h7/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h8/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h8/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h8/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h8/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h8/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h8/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h9/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h9/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h9/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h9/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h9/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h9/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/wpe                                                     size 4194304      slice_size 2097152      Shape[embed_sequence=2048, embd=2048]                       \n",
            "Variable gpt2/wte                                                     size 102926336    slice_size 51463168     Shape[vocab=50257, embd=2048]                               \n",
            "Variable stacked/gpt2/h0/mlp/conv1d_main/c_fc/bias                    size 196608       slice_size 49152        Shape[stacked=24, intermediate_expanded=8192]               \n",
            "    gpt2/h0/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h1/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h2/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h3/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h4/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h5/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h6/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h7/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h8/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h9/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h10/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h11/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h12/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h13/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h14/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h15/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h16/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h17/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h18/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h19/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h20/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h21/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h22/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h23/mlp/conv1d_main/c_fc/bias\n",
            "Variable stacked/gpt2/h0/norm_1/g                                     size 131072       slice_size 65536        Shape[stacked=64, embd=2048]                                \n",
            "    gpt2/h0/norm_1/g\n",
            "    gpt2/h0/norm_1/b\n",
            "    gpt2/h0/attn/compute_output_bias/o_b\n",
            "    gpt2/h0/norm_2/g\n",
            "    gpt2/h0/norm_2/b\n",
            "    gpt2/h0/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h1/norm_1/g\n",
            "    gpt2/h1/norm_1/b\n",
            "    gpt2/h1/attn/compute_output_bias/o_b\n",
            "    gpt2/h1/norm_2/g\n",
            "    gpt2/h1/norm_2/b\n",
            "    gpt2/h1/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h2/norm_1/g\n",
            "    gpt2/h2/norm_1/b\n",
            "    gpt2/h2/attn/compute_output_bias/o_b\n",
            "    gpt2/h2/norm_2/g\n",
            "    gpt2/h2/norm_2/b\n",
            "    gpt2/h2/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h3/norm_1/g\n",
            "    gpt2/h3/norm_1/b\n",
            "    gpt2/h3/attn/compute_output_bias/o_b\n",
            "    gpt2/h3/norm_2/g\n",
            "    gpt2/h3/norm_2/b\n",
            "    gpt2/h3/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h4/norm_1/g\n",
            "    gpt2/h4/norm_1/b\n",
            "    gpt2/h4/attn/compute_output_bias/o_b\n",
            "    gpt2/h4/norm_2/g\n",
            "    gpt2/h4/norm_2/b\n",
            "    gpt2/h4/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h5/norm_1/g\n",
            "    gpt2/h5/norm_1/b\n",
            "    gpt2/h5/attn/compute_output_bias/o_b\n",
            "    gpt2/h5/norm_2/g\n",
            "    gpt2/h5/norm_2/b\n",
            "    gpt2/h5/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h6/norm_1/g\n",
            "    gpt2/h6/norm_1/b\n",
            "    gpt2/h6/attn/compute_output_bias/o_b\n",
            "    gpt2/h6/norm_2/g\n",
            "    gpt2/h6/norm_2/b\n",
            "    gpt2/h6/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h7/norm_1/g\n",
            "    gpt2/h7/norm_1/b\n",
            "    gpt2/h7/attn/compute_output_bias/o_b\n",
            "    gpt2/h7/norm_2/g\n",
            "    gpt2/h7/norm_2/b\n",
            "    gpt2/h7/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h8/norm_1/g\n",
            "    gpt2/h8/norm_1/b\n",
            "    gpt2/h8/attn/compute_output_bias/o_b\n",
            "    gpt2/h8/norm_2/g\n",
            "    gpt2/h8/norm_2/b\n",
            "    gpt2/h8/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h9/norm_1/g\n",
            "    gpt2/h9/norm_1/b\n",
            "    gpt2/h9/attn/compute_output_bias/o_b\n",
            "    gpt2/h9/norm_2/g\n",
            "    gpt2/h9/norm_2/b\n",
            "    gpt2/h9/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h10/norm_1/g\n",
            "    gpt2/h10/norm_1/b\n",
            "    gpt2/h10/attn/compute_output_bias/o_b\n",
            "    gpt2/h10/norm_2/g\n",
            "Variable stacked/gpt2/h10/norm_2/b                                    size 131072       slice_size 65536        Shape[stacked=64, embd=2048]                                \n",
            "    gpt2/h10/norm_2/b\n",
            "    gpt2/h10/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h11/norm_1/g\n",
            "    gpt2/h11/norm_1/b\n",
            "    gpt2/h11/attn/compute_output_bias/o_b\n",
            "    gpt2/h11/norm_2/g\n",
            "    gpt2/h11/norm_2/b\n",
            "    gpt2/h11/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h12/norm_1/g\n",
            "    gpt2/h12/norm_1/b\n",
            "    gpt2/h12/attn/compute_output_bias/o_b\n",
            "    gpt2/h12/norm_2/g\n",
            "    gpt2/h12/norm_2/b\n",
            "    gpt2/h12/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h13/norm_1/g\n",
            "    gpt2/h13/norm_1/b\n",
            "    gpt2/h13/attn/compute_output_bias/o_b\n",
            "    gpt2/h13/norm_2/g\n",
            "    gpt2/h13/norm_2/b\n",
            "    gpt2/h13/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h14/norm_1/g\n",
            "    gpt2/h14/norm_1/b\n",
            "    gpt2/h14/attn/compute_output_bias/o_b\n",
            "    gpt2/h14/norm_2/g\n",
            "    gpt2/h14/norm_2/b\n",
            "    gpt2/h14/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h15/norm_1/g\n",
            "    gpt2/h15/norm_1/b\n",
            "    gpt2/h15/attn/compute_output_bias/o_b\n",
            "    gpt2/h15/norm_2/g\n",
            "    gpt2/h15/norm_2/b\n",
            "    gpt2/h15/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h16/norm_1/g\n",
            "    gpt2/h16/norm_1/b\n",
            "    gpt2/h16/attn/compute_output_bias/o_b\n",
            "    gpt2/h16/norm_2/g\n",
            "    gpt2/h16/norm_2/b\n",
            "    gpt2/h16/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h17/norm_1/g\n",
            "    gpt2/h17/norm_1/b\n",
            "    gpt2/h17/attn/compute_output_bias/o_b\n",
            "    gpt2/h17/norm_2/g\n",
            "    gpt2/h17/norm_2/b\n",
            "    gpt2/h17/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h18/norm_1/g\n",
            "    gpt2/h18/norm_1/b\n",
            "    gpt2/h18/attn/compute_output_bias/o_b\n",
            "    gpt2/h18/norm_2/g\n",
            "    gpt2/h18/norm_2/b\n",
            "    gpt2/h18/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h19/norm_1/g\n",
            "    gpt2/h19/norm_1/b\n",
            "    gpt2/h19/attn/compute_output_bias/o_b\n",
            "    gpt2/h19/norm_2/g\n",
            "    gpt2/h19/norm_2/b\n",
            "    gpt2/h19/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h20/norm_1/g\n",
            "    gpt2/h20/norm_1/b\n",
            "    gpt2/h20/attn/compute_output_bias/o_b\n",
            "    gpt2/h20/norm_2/g\n",
            "    gpt2/h20/norm_2/b\n",
            "    gpt2/h20/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h21/norm_1/g\n",
            "    gpt2/h21/norm_1/b\n",
            "Variable stacked/gpt2/h21/attn/compute_output_bias/o_b                size 36864        slice_size 18432        Shape[stacked=18, embd=2048]                                \n",
            "    gpt2/h21/attn/compute_output_bias/o_b\n",
            "    gpt2/h21/norm_2/g\n",
            "    gpt2/h21/norm_2/b\n",
            "    gpt2/h21/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h22/norm_1/g\n",
            "    gpt2/h22/norm_1/b\n",
            "    gpt2/h22/attn/compute_output_bias/o_b\n",
            "    gpt2/h22/norm_2/g\n",
            "    gpt2/h22/norm_2/b\n",
            "    gpt2/h22/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h23/norm_1/g\n",
            "    gpt2/h23/norm_1/b\n",
            "    gpt2/h23/attn/compute_output_bias/o_b\n",
            "    gpt2/h23/norm_2/g\n",
            "    gpt2/h23/norm_2/b\n",
            "    gpt2/h23/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/ln_f/g\n",
            "    gpt2/ln_f/b\n",
            "Trainable Variables            count: 150     Total size: 1315575808       Total slice_size: 204753920      \n",
            "All Variables                  count: 150     Total size: 1315575808       Total slice_size: 204753920      \n",
            "Counters:\n",
            "allreduce: 2.02e+10\n",
            " allreduce/[0]: 6.45e+09\n",
            "  allreduce/[0]/einsum_op: 6.45e+09\n",
            " allreduce/[1]: 1.37e+10\n",
            "  allreduce/[1]/einsum_op: 1.37e+10\n",
            "  allreduce/[1]/reduce_op: 2.54e+07\n",
            "einsum: 3.67e+13\n",
            "einsum_unique: 2.57e+13\n",
            "output: 2.65e+11\n",
            " output/AddOperation: 6.82e+10\n",
            " output/BinaryOpWithBroadcasting: 5.2e+08\n",
            " output/BroadcastOperation: 6.48e+09\n",
            " output/ConcatOperation: 3.22e+09\n",
            " output/Constant: 1.97e+05\n",
            " output/EinsumOperation: 6.98e+10\n",
            " output/ImportOperation: 2.62e+05\n",
            " output/OneHotOperation: 6.62e+09\n",
            " output/RangeOperation: 2.54e+05\n",
            " output/ReduceOperation: 3.8e+07\n",
            " output/ReshapeOperation: 1.21e+10\n",
            " output/ScalarAddOperation: 6.45e+09\n",
            " output/ScalarMultiplyOperation: 2.27e+10\n",
            " output/ShiftOperation: 1.61e+09\n",
            " output/SlicewiseOperation: 5.24e+10\n",
            " output/StackedVariable: 1.59e+06\n",
            " output/StopGradient: 9.66e+09\n",
            " output/UnstackOperation: 1.59e+06\n",
            " output/Variable: 1.64e+09\n",
            " output/WhileLoopOperation: 3.22e+09\n",
            "output_unique: 1.46e+11\n",
            " output_unique/AddOperation: 3.72e+10\n",
            " output_unique/BinaryOpWithBroadcasting: 6.72e+07\n",
            " output_unique/BroadcastOperation: 6.45e+09\n",
            " output_unique/ConcatOperation: 1.61e+09\n",
            " output_unique/Constant: 2.46e+04\n",
            " output_unique/EinsumOperation: 3.07e+10\n",
            " output_unique/ImportOperation: 3.28e+04\n",
            " output_unique/OneHotOperation: 8.28e+08\n",
            " output_unique/RangeOperation: 3.28e+04\n",
            " output_unique/ReduceOperation: 1.42e+07\n",
            " output_unique/ReshapeOperation: 6.44e+09\n",
            " output_unique/ScalarAddOperation: 3.22e+09\n",
            " output_unique/ScalarMultiplyOperation: 1.05e+10\n",
            " output_unique/ShiftOperation: 8.05e+08\n",
            " output_unique/SlicewiseOperation: 3.75e+10\n",
            " output_unique/StackedVariable: 4.96e+05\n",
            " output_unique/StopGradient: 8.05e+09\n",
            " output_unique/UnstackOperation: 4.96e+05\n",
            " output_unique/Variable: 1.32e+09\n",
            " output_unique/WhileLoopOperation: 1.61e+09\n",
            "variables: 1.32e+09\n",
            " variables/trainable: 1.32e+09\n",
            "Done calling model_fn.\n",
            "TPU job name worker\n",
            "Graph was finalized.\n",
            "Restoring parameters from gs://ditto_gptneo/GPT3_XL/model.ckpt-362000\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:840: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "Starting infeed thread controller.\n",
            "Starting outfeed thread controller.\n",
            "Initialized dataset iterators in 0 seconds\n",
            "Before copy master to slices.\n",
            "Done with copy master to slices.\n",
            "Enqueue next (1) batch(es) of data to infeed.\n",
            "Dequeue next (1) batch(es) of data from outfeed.\n",
            "Outfeed finished for iteration (0, 0)\n",
            "======================================== SAMPLE 0 ========================================\n",
            "\n",
            "\n",
            "class GPT(nn.Module):\n",
            "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
            "\n",
            "    def __init__(self, config):\n",
            "        super().__init__()\n",
            "\n",
            "        # input embedding stem\n",
            "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
            "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
            "        self.drop = nn.Dropout(config.embd_pdrop)\n",
            "        # transformer\n",
            "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
            "        # decoder head\n",
            "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
            "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
            "\n",
            "        self.block_size = config.block_size\n",
            "        self.apply(self._init_weights)\n",
            "\n",
            "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
            "\n",
            "\n",
            "    def _init_weights(self):\n",
            "        self.weights.append(nn.Parameter(torch.zeros(1, (1,1))) if config.n_layer > 1 else nn.Parameter(torch.rand(1, config.block_size, self.block_size)) if config.n_layer == 1 else nn.Parameter(torch.rand(1, self.block_size, config.n_embd ** 2) * (1-self.tok_emb.numel()/config.block_size)))\n",
            "\n",
            "        if self.tok_emb.numel() > 0:\n",
            "            self.tok_emb = Module(self.tok_emb)\n",
            "\n",
            "        if config.max_layers > 0:\n",
            "            # for each llayer\n",
            "            self.layers = []\n",
            "            for i in range(config.max_layers):\n",
            "                if i == 0:\n",
            "                    self.layers.append(self.layers[-1])\n",
            "                else:\n",
            "                    self.layers.append(nn.Linear(config.n_layer, config.vocab_size))\n",
            "\n",
            "    def get_states(self, input, context, b = True, o = None):\n",
            "        states = []\n",
            "        states.extend(self.layers(input, context, o))\n",
            "        return states\n",
            "\n",
            "    def reset_states(self):\n",
            "        for l in self.layers:\n",
            "            l.copy_(self.weights, state=torch.stack([])).fill_(1)\n",
            "        self.layers = [self.layers[i] for i in range(self.layers.size()) if config.n_layer > i]\n",
            "\n",
            "    def forward(self, tokens):\n",
            "        state = self.get_states(tokens, self.tok_emb).repeat(config.n_layers)\n",
            "        # return tokens * state\n",
            "        return self.apply(self._transform_sequence(state, self.drop))\n",
            "\n",
            "    def forward_inplace(self, inputs):\n",
            "        for i in range(len(self.layers)):\n",
            "            v = self.layers[i]\n",
            "            l = self.layers[i - 1]\n",
            "            states = self.get_states(inputs[i], l.input, b = True, o = None)\n",
            "            v.copy_(states, state=torch.stack([])).fill_(1)\n",
            "        return inputs\n",
            "\n",
            "    def apply(self, input):\n",
            "        if config.fuse:\n",
            "            return self.fc(input, self.tok_embed).reshape(-1)  # vias\n",
            "        else:\n",
            "            if config.n_layer == 1:\n",
            "                self.weight.zero_()\n",
            "                self.bias.zero_()\n",
            "            else:\n",
            "            # we feed models multiple times with different vocab sizes\n",
            "                self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
            "                for _ in range(config.max_layers):\n",
            "                    self.bias = nn.LayerNorm(config.n_embd**2)\n",
            "                    self.tok_emb = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
            "                    self.weight = Module(self.tok_emb)\n",
            "\n",
            "            # transformer\n",
            "            self.blocks = self.layers(self.weight, self.bias, self.tok_emb)\n",
            "\n",
            "        if config.max_layers > 0:\n",
            "            # for each llayer\n",
            "            self.tokens = torch.tensor([t for t, _ in zip(self.blocks, self.tok_emb)], device='cuda')\n",
            "\n",
            "        return self._do_init(*self.blocks)\n",
            "\n",
            "\n",
            "class GPTInferOpOperator(nn.Module):\n",
            "    \"\"\" GPTInferOp, which is a counterpart of INferOp for GPT\"\"\"\n",
            "\n",
            "    def __init__(self, config):\n",
            "        super().__init__()\n",
            "\n",
            "        # input embedding stem\n",
            "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
            "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
            "        self.drop = nn.Dropout(config.embd_pdrop)\n",
            "\n",
            "        self.fuse = nn.FullyConnected(config.n_layer if config.n_layer > 0 else config.block_size)\n",
            "        self.apply(self._init_weights)\n",
            "\n",
            "        layers = []\n",
            "        self.layers = [self.layers[i] for i in range(config.max_layers)]\n",
            "\n",
            "        for i in range(len(self.layers)):\n",
            "            layers.append(nn.Linear(config.n_layer, config.vocab_size))\n",
            "\n",
            "        self.weight = nn.Parameter(torch.zeros(1, dim=config\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "\n",
            "class GPT(nn.Module):\n",
            "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
            "\n",
            "    def __init__(self, config):\n",
            "        super().__init__()\n",
            "\n",
            "        # input embedding stem\n",
            "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
            "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
            "        self.drop = nn.Dropout(config.embd_pdrop)\n",
            "        # transformer\n",
            "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
            "        # decoder head\n",
            "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
            "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
            "\n",
            "        self.block_size = config.block_size\n",
            "        self.apply(self._init_weights)\n",
            "\n",
            "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
            "\n",
            "    def _init_weights(self):\n",
            "        \"\"\"  initialize the weight matrices which will be used in the forward solver \"\"\"\n",
            "        # directly map from vocab_size to layer params\n",
            "        for i, p in enumerate(self.parameters()):\n",
            "            self.parameter(torch.max(p.numel(), 1))\n",
            "\n",
            "    #clerical parameters, need to be set outside of class, in order to be passed to forward solver\n",
            "    def _set_ndim(self):\n",
            "        \"set the shape of the first layer of the decoder, i.e. the leading dimension of the convolutional\n",
            "        tensors in forward solver\"\n",
            "        for p in self.parameters():\n",
            "            self.nn.set_param(p, (ind.int64()//config.block_size,), shape=(1,) + p.numel(), relu=False)\n",
            "        return\n",
            "\n",
            "\n",
            "    def forward(self, x:torch.Tensor, y:torch.Tensor) -> torch.Tensor:\n",
            "        \"\"\"  transform examples into a probability distribution \"\"\"\n",
            "\n",
            "        # flatten x tensor\n",
            "        x = x.view(x.size(0), -1)\n",
            "        n_ch = x.batch_shape[-2:]\n",
            "        shape1 = x.max()\n",
            "\n",
            "        layer_f = self.modules[0]\n",
            "\n",
            "        # gpu memory\n",
            "        y_f = self.block_f.forward(x, y)\n",
            "\n",
            "        # save weights for forward solver\n",
            "        self.fc_weights = torch.Tensor(layer_f(y, self.layer_f))\n",
            "        self.weight_mapping_f = nn.ModuleList()\n",
            "        for i,p in enumerate(self.parameters()):\n",
            "            weight_mapping_f[i] = nn.ModuleList()\n",
            "            weight_mapping_f[i].int64() = p.numel()\n",
            "            weight_mapping_f[i].resize((p.dim()-1.),1,1)\n",
            "            weight_mapping_f[i].weight.requires_grad = False\n",
            "            weight_mapping_f[i].bias = p.bias\n",
            "            weight_mapping_f[i].copy_(self.fc_weights[i])\n",
            "            weight_mapping_f[i].requires_grad = self.fc_weights[i].requires_grad\n",
            "            weight_mapping_f[i].copy_(self.weight_mapping_f[i])\n",
            "            weight_mapping_f[i].shape.dim() = config.block_size\n",
            "            layer_f.apply(weight_mapping_f[i])\n",
            "\n",
            "        # get decoder weights\n",
            "        self.decoder_weights = self.module_param_to_weight_map[config.word_encoder]\n",
            "\n",
            "        # get output tensors\n",
            "        # output\n",
            "        # y_out\n",
            "        #    fc0\n",
            "        #        (1) x\n",
            "        #        (2) y\n",
            "        #\n",
            "        # output\n",
            "        #    fc1\n",
            "        #        (1) x\n",
            "        #        (2) x\n",
            "        #        (3) fc0\n",
            "        #        (4) y\n",
            "        #        (5) fc1\n",
            "\n",
            "        # flatten both outputs\n",
            "        y_f_1 = y_f.view_as(y_f)\n",
            "        y_f_2 = y_f_1.view_as(y_f)\n",
            "        y_f_out = y_f_2.view_as(y_f_1).permute(1,2,0,3,4,2,3,1)\n",
            "\n",
            "        # compute output biases\n",
            "        if self.block_size == config.block_size:\n",
            "            self.loc_f = layer_f(y_out[:, y_f_out.size(2):], self.layer_f)\n",
            "        else:\n",
            "            self.loc_f = self.block_f(y_f_out, self.layer_f)\n",
            "\n",
            "        self._soft_max_y_f_out = torch.nn.Batchnorm(y_f_out, self.loc_f)\n",
            "\n",
            "        return y_f_out\n",
            "\n",
            "    def _rescale(self, y:torch.Tensor, scale:torch.Tensor):\n",
            "        \"\"\"  perform a scale on the decoder output \"\"\"\n",
            "        return self.decoder_weights * scale * self.soft_max_y_f_out\n",
            "\n",
            "\n",
            "class WordEmbedding(nn.Module):\n",
            "    def __init__(self, config):\n",
            "        super().__init__()\n",
            "\n",
            "        self.data = nn.ModuleList()\n",
            "        self.sstem = nn.Parameter(torch.zeros(1, 1, config.vocab_size))\n",
            "        self.spos = nn.Parameter(torch.zeros(1, config.n_embd))\n",
            "        self.drop = nn.Dropout(config.embd_pdrop)\n",
            "        self.lbase = nn.Parameter(torch.zeros(1, config.n_embd))\n",
            "        self.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 2 ========================================\n",
            "\n",
            "\n",
            "class GPT(nn.Module):\n",
            "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
            "\n",
            "    def __init__(self, config):\n",
            "        super().__init__()\n",
            "\n",
            "        # input embedding stem\n",
            "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
            "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
            "        self.drop = nn.Dropout(config.embd_pdrop)\n",
            "        # transformer\n",
            "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
            "        # decoder head\n",
            "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
            "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
            "\n",
            "        self.block_size = config.block_size\n",
            "        self.apply(self._init_weights)\n",
            "\n",
            "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
            "\n",
            "    def _init_weights(self):\n",
            "        # self.vocab_size = config.vocab_size\n",
            "        # self.n_embd = config.n_embd\n",
            "        # self.n_head   = config.n_head\n",
            "        # self.n_layer  = config.n_layer\n",
            "        # self.n_head_new = config.n_head\n",
            "        # self.s00      = (cfg.size(0) - config.block_size - 1) / config.block_size - 1\n",
            "        # self.s01      = (cfg.size(1) - config.block_size - 1) / config.block_size - 1\n",
            "        # self.s02      = (cfg.size(2) - config.block_size - 1) / config.block_size - 1\n",
            "        # self.embd     = self.s01 + self.s02\n",
            "\n",
            "    def forward(self, x):\n",
            "        blocks_sz = self.s00\n",
            "        fc = blocks_sz * self.embd(x)\n",
            "        if self.s01 >= 0:\n",
            "            fe = self.s01\n",
            "        else:\n",
            "            fe = self.s02\n",
            "        xout = blocks_sz * ((fc - self.ln_f(fc)) / self.ln_f(fe))\n",
            "        return xout\n",
            "\n",
            "\n",
            "class CERTF(nn.Module):\n",
            "    \"\"\"  The Context Entropy Regularized Linear Classification Model\n",
            "\n",
            "   .. math::\n",
            "\n",
            "        E = - \\frac{1}{2} \\sum_{l=1}^T {\\left( {y_{\\mathrm{cl,l}} - \\hat{y}_{\\mathrm{cl,l}}} \\right)}^2\n",
            "\n",
            "    where T is the number of time steps. The loss function is\n",
            "    :math:`L_{\\mathrm{cERTF}} = \\mathrm{Backward_{T}}^{L_{\\mathrm{cERTF}}} `\n",
            "    and the regularization term is :math:`L_{\\mathrm{cERTF}} `\n",
            "\n",
            "    This class takes a sequence of logs, the target sequence y_cl. We solve this\n",
            "    class problem with a small learning rate.\n",
            "\n",
            "   .. math::\n",
            "\n",
            "        E = - \\frac{1}{2} \\sum_{l=1}^T {\\left( {y_{\\mathrm{cl,l}} - y_{\\mathrm{cl,l} + 1}} \\right)}^2\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "class BatchCERTF(nn.Module):  # Batch will be initialized on first time to zero\n",
            "    \"\"\"  The Batch Context-Entropy Regularized Linear Classifier Model\n",
            "\n",
            "   .. math::\n",
            "\n",
            "        E = - \\frac{1}{2} \\sum_{l=1}^T {\\mathrm{Entropy_{T}}}\\left( {\\mathbf{y},\\mathbf{y} + t_{l}} \\right)\n",
            "\n",
            "    where T is the number of time steps. The loss function is\n",
            "    :math:`L_{cBatch} = \\mathrm{Backward_{T}}^{L_{\\mathrm{cERTF}}} `\n",
            "    and the regularization term is :math:`L_{cBatch} `\n",
            "\n",
            "    This class takes a batch of logs, the target sequence y_. We solve this\n",
            "    class problem with a small learning rate.\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, batch_size, n_logits, lr_decay, n_classes):\n",
            "        super().__init__()\n",
            "        self.batch_size = batch_size\n",
            "        self.n_logits = n_logits\n",
            "        self.lr_decay = lr_decay\n",
            "        self.n_classes = n_classes\n",
            "        self.logits = nn.Sequential(\n",
            "            Block(BatchCERTF(batch_size, n_logits, self.lr_decay, n_classes)),\n",
            "        )\n",
            "\n",
            "    def _init_weights(self):\n",
            "        self.h158 = self.logits.half_max(0.5)\n",
            "        self.h172 = self.logits.half_max(0.5)\n",
            "        self.kernel = torch.ones(self.batch_size, self.kernel_size, nn.Parameter(torch.Tensor(1, 1)))\n",
            "        self.weights = nn.Parameter(torch.zeros(self.batch_size, self.kernel_size, requires_grad=True))\n",
            "\n",
            "    def forward(self, x):\n",
            "        x_Translate = torch.zeros(self.batch_size, self.kernel, requires_grad=True)\n",
            "        x_Translate = x_Translate.unsqueeze(0)\n",
            "        x_Translate = x_Translate.unsqueeze(1)\n",
            "        x_Translate = x_Translate.unsqueeze(2)\n",
            "        for t in range(self.n_classes):\n",
            "            x_Translate = self.logits(x_Translate.unsqueeze(2))\n",
            "            x_Translate = x_Translate.unsqueeze(0)\n",
            "            x_Translate = x_Translate.unsqueeze(1)\n",
            "            logits = self.h158(x_Translate)\n",
            "            logits = logits.unsqueeze(0)\n",
            "            logits = logits.unsqueeze(1)\n",
            "            logits = torch.matmul(logits, self.kernel)\n",
            "            logits = logits.unsqueeze(0)\n",
            "            logits = logits.unsqueeze(1)\n",
            "            logits = logits.unsqueeze(2)\n",
            "\n",
            " \n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 3 ========================================\n",
            "\n",
            "\n",
            "class GPT(nn.Module):\n",
            "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
            "\n",
            "    def __init__(self, config):\n",
            "        super().__init__()\n",
            "\n",
            "        # input embedding stem\n",
            "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
            "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
            "        self.drop = nn.Dropout(config.embd_pdrop)\n",
            "        # transformer\n",
            "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
            "        # decoder head\n",
            "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
            "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
            "\n",
            "        self.block_size = config.block_size\n",
            "        self.apply(self._init_weights)\n",
            "\n",
            "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
            "        logger.info(\"number of shared summation weights: %e\", self.shared_weight_size)\n",
            "\n",
            "\n",
            "def export_graph(config, name):\n",
            "    \"\"\"  generate an embedding and a token embedding out of the previous graph \"\"\"\n",
            "\n",
            "    graph = nn.Graph()\n",
            "    graph.add_node(config.n_layer, nn.ModuleList())([\"transformer\", \"transformer\", \"\"])\n",
            "    graph.add_node(config.n_layer + 1, nn.ModuleList())([\"module\", \"module\", \"\"])\n",
            "    graph.add_node(config.n_layer, nn.ModuleList())([\"lats\", \"lats\", \"\"])\n",
            "    graph.add_node(config.n_layer, nn.ModuleList())([\"fc\", \"fc\", \"\"])\n",
            "    graph.add_node(config.n_layer, nn.ModuleList())([\"decoder\", \"decoder\", \"\"])\n",
            "    graph.add_node(config.n_layer, nn.ModuleList())([\"ln_f\", \"ln_f\", \"\"])\n",
            "\n",
            "    embeddings = []\n",
            "    for _ in range(config.n_layers):\n",
            "        token_emb = embeddings.extend(embeddings[self.params[-1].numel() - 1] * config.block_size)\n",
            "        embeddings.append(token_emb)\n",
            "    embeddings = torch.cat(embeddings, 1)\n",
            "    # token_emb.requires_grad = True\n",
            "    #sentence_emb.requires_grad = True\n",
            "\n",
            "    z = torch.cat(embeddings, 1)\n",
            "    logger.info(\"initialized tokens and sentence embeddings: %e\", z.numel())\n",
            "    graph_final.export_graph(config, name)\n",
            "\n",
            "    return {'graph': graph}\n",
            "\n",
            "def load_graph(config, name):\n",
            "    return load_graph_from_ckpt(config.filename, graph=None)\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 4 ========================================\n",
            "\n",
            "\n",
            "class GPT(nn.Module):\n",
            "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
            "\n",
            "    def __init__(self, config):\n",
            "        super().__init__()\n",
            "\n",
            "        # input embedding stem\n",
            "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
            "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
            "        self.drop = nn.Dropout(config.embd_pdrop)\n",
            "        # transformer\n",
            "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
            "        # decoder head\n",
            "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
            "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
            "\n",
            "        self.block_size = config.block_size\n",
            "        self.apply(self._init_weights)\n",
            "\n",
            "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
            "        self.init_block_norm()\n",
            "\n",
            "        self.classifier = Classifier(config)\n",
            "\n",
            "    def forward(self, xs, mask):\n",
            "        emb = self.tok_emb(xs)\n",
            "        pos_emb = self.pos_emb(xs)\n",
            "        self.pos_emb = torch.autograd.Variable(pos_emb.view(emb.size()[0], 1, -1, emb.size()[1]))\n",
            "        self.emb = emb.unsqueeze(2)\n",
            "        self.n_emb = self.emb.size()\n",
            "        context_emb = self.emb(context_size=1, batch_size=6)\n",
            "        # current context\n",
            "        self.current_context,  # [batch]\n",
            "        self.current_context = context_emb\n",
            "        # combine current context to self.current_context\n",
            "        if self.batch:\n",
            "            res = xs.unsqueeze(0).expand_as(self.current_context)\n",
            "            if mask.any():\n",
            "                self.current_context = self.current_context[mask, :].expand_as(context_emb)\n",
            "                res.add_(0, 0.0)\n",
            "            return res\n",
            "\n",
            "        n_start_for_current_context = self.current_context.size(1)\n",
            "        if config.temporal_average:\n",
            "            # examine if the current context spans for the target vary\n",
            "            # the number of frames hinting for temp context to consider\n",
            "            if self.batch:\n",
            "                start_for_temp_context, end_for_current_context = start_for_current_context.view(1, config.batch)\n",
            "                temp_context = self.current_context[start_for_temp_context, end_for_current_context].expand_as(self.current_context)\n",
            "                start_skip_current_context, end_skip_current_context = self.current_context.view(1, config.batch)\n",
            "                temp_context[start_skip_current_context, end_skip_current_context] = 1.0\n",
            "                n_start_for_temp_context = temp_context.size(1)\n",
            "\n",
            "        self.forward(context_emb, mask=mask.view(self.batch, max_out_emb(context_emb, self.n_emb), 1))\n",
            "        return self.current_context\n",
            "\n",
            "    def init_block_norm(self):\n",
            "        \"\"\"\n",
            "        this helps the block nose be able to build multiple encoder blocks with\n",
            "        same parameters. If any parameter changes are made in the next layers(s) of the model this will be\n",
            "        synchronized with the first blocks\n",
            "        \"\"\"\n",
            "        self.tcx, self.cx = nn.Variable(torch.ones((self.block_size, 1, 1, 1)), bias=True)\n",
            "        self.f_coef = self.cx\n",
            "        self.rois, self.tcx_f = nn.Constant(torch.ones((1, self.block_size, 1, 1)), bias=True)\n",
            "        self.o2f_coef, self.o8f_coef = nn.Constant(torch.zeros((1, self.block_size, 1, 1)), bias=True)\n",
            "        self.l2f_coef, self.l8f_coef = nn.Constant(torch.zeros((1, self.block_size, 1, 1)), bias=True)\n",
            "        # merge blocks with similarity of final layers of the head\n",
            "        self.merge_coefs = nn.MergeBlock(self.cx, self.tcx_f, self.tcx=self.tcx, self.l8f=self.l8f_coef, self.o2f=self.o2f_coef)\n",
            "        # concatenate transformer and decoder head\n",
            "        self.tcx_c = nn.Concatenate(self.tcx_f, self.merge_coefs)\n",
            "        self.fw = nn.Linear(self.tcx_c.size(0), self.block_size, bias=False)\n",
            "        self.o2f_c = nn.Linear(self.tcx_c.size(0), self.block_size, bias=True)\n",
            "        self.l2f_c = nn.Linear(self.tcx_c.size(0), self.block_size, bias=True)\n",
            "        #\n",
            "        # these variables do the same as normal nets\n",
            "        # columns work just as what has been done for pre-gru results\n",
            "        self.trans = nn.Link(self.tcx_c, self.f_coef)\n",
            "        self.pre = nn.Linear(self.tcx_c.size(0), self.pre)\n",
            "        self.post = nn.Linear(self.tcx_c.size(0), self.post)\n",
            "        self.l_fc = nn.Linear(self.tcx_c.size(0), self.block_size)\n",
            "        self.o_fc = nn.Linear(self.tcx_c.size(0), self.l_fc)\n",
            "        self.o_fc_c = nn.Linear(self.tcx_c.size(0), self.o2f_c)\n",
            "        self.o_fc_fc = nn.Linear(self.tcx_c.size(\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 5 ========================================\n",
            "\n",
            "\n",
            "class GPT(nn.Module):\n",
            "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
            "\n",
            "    def __init__(self, config):\n",
            "        super().__init__()\n",
            "\n",
            "        # input embedding stem\n",
            "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
            "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
            "        self.drop = nn.Dropout(config.embd_pdrop)\n",
            "        # transformer\n",
            "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
            "        # decoder head\n",
            "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
            "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
            "\n",
            "        self.block_size = config.block_size\n",
            "        self.apply(self._init_weights)\n",
            "\n",
            "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
            "\n",
            "    def _init_weights():\n",
            "        \"\"\" initializes the parameters of the model. \"\"\"\n",
            "        nn.init.normal_(self.tok_emb.weight.data, std=0.001)\n",
            "        nn.init.normal_(self.pos_emb.weight.data, std=0.001)\n",
            "        nn.init.constant_(self.drop, 0.1)\n",
            "        nn.init.normal_(self.ln_f.weight.data, mean=0., std=0.1)\n",
            "\n",
            "    def forward(self, k):\n",
            "        \"\"\"\n",
            "        pass                         # actually passes the k tensors\n",
            "        \"\"\"\n",
            "        t_input = self.tok_emb(k, self.config)\n",
            "        p_input = self.pos_emb(k, self.config)\n",
            "\n",
            "        q_f = self.head(t_input)\n",
            "        if self.config.n_layer % self.config.ALPHA == 0:\n",
            "            out = self.ln_f(q_f.log())\n",
            "            weight = self.library_block(k, p_input, q_f.log())\n",
            "            self.library_block = other_blocks[int(self.config.ALPHA)]\n",
            "        else:\n",
            "            out = self.ln_f(q_f.log())\n",
            "            weight = self.library_block(k, p_input, q_f)\n",
            "            self.library_block = other_blocks[int(self.config.ALPHA)]\n",
            "        # autoencoder\n",
            "        self.bq = self.ln_f(out)\n",
            "        self.bq = self.ln_f(self.bq)\n",
            "\n",
            "        return weight, t_input, p_input\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 6 ========================================\n",
            "\n",
            "\n",
            "class GPT(nn.Module):\n",
            "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
            "\n",
            "    def __init__(self, config):\n",
            "        super().__init__()\n",
            "\n",
            "        # input embedding stem\n",
            "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
            "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
            "        self.drop = nn.Dropout(config.embd_pdrop)\n",
            "        # transformer\n",
            "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
            "        # decoder head\n",
            "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
            "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
            "\n",
            "        self.block_size = config.block_size\n",
            "        self.apply(self._init_weights)\n",
            "\n",
            "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
            "\n",
            "    def _init_weights(self):\n",
            "        \"\"\" init weights\n",
            "\n",
            "        def init_weights(self):\n",
            "            self.conv1 = nn.Conv2d(2, self.block_size, padding=padding)\n",
            "            self.fc1 = nn.Linear(self.block_size, self.vocab_size, bias=False)\n",
            "            self.fc1.weight.data.copy_(self.fc1.weight.data)\n",
            "            self.conv2 = nn.Conv2d(self.block_size, self.block_size, padding=1, kernel_size=5)\n",
            "            self.conv2 = nn.LeakyReLU(relu=False)\n",
            "            self.conv3 = nn.Conv2d(self.block_size, self.block_size, kernel_size=3, bias=False)\n",
            "            self.fc2 = nn.Linear(self.block_size, self.block_size, bias=False)\n",
            "            self.fc2.weight.data.copy_(self.fc2.weight.data)\n",
            "            self.fc3 = nn.Linear(self.block_size, self.block_size, bias=False)\n",
            "            self.fc3.weight.data.copy_(self.fc3.weight.data)\n",
            "\n",
            "            self.fc3.weight.copy_(self.fc3.b.data)\n",
            "            self.fc4 = nn.Linear(self.block_size, self.block_size, bias=False)\n",
            "            self.fc4.b.weight.data.copy_(self.fc4.b.data)\n",
            "            self.fc5 = nn.Linear(self.block_size, self.block_size, bias=False)\n",
            "            self.fc5.b.weight.data.copy_(self.fc5.b.data)\n",
            "\n",
            "            self.fc5.b.bias.data = self.fc3.b.bias.data\n",
            "            self.fc6 = nn.Linear(self.block_size, self.block_size, bias=False)\n",
            "            self.fc6.b.bias.data = self.fc3.b.bias.data\n",
            "            self.fc7 = nn.Linear(self.block_size, self.block_size, bias=False)\n",
            "            self.fc7.b.bias.data = self.fc3.b.bias.data\n",
            "            self.fc8 = nn.Linear(self.block_size, self.block_size, bias=False)\n",
            "            self.fc8.b.bias.data = self.fc3.b.bias.data\n",
            "\n",
            "        def forward(self, x):\n",
            "            return self.conv1(x)\n",
            "        def backward(self, x, grad_output):\n",
            "            for p in self.parameters():\n",
            "                if grad_output[p.name] > 0:\n",
            "                    x = self.fc1(x)\n",
            "                    x = self.fc2(x)\n",
            "                    out = self.fc3(x)\n",
            "                    out = self.fc4(out)\n",
            "                    out = self.fc5(out)\n",
            "                    out = self.fc6(out)\n",
            "                    out = self.fc7(out)\n",
            "                    out = self.fc8(out)\n",
            "                    out = grad_output[p.name] * out\n",
            "                    out = grad_output[p.name] * out\n",
            "\n",
            "                    x = x - out * grad_output[p.name]\n",
            "\n",
            "        return x\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 7 ========================================\n",
            "\n",
            "\n",
            "class GPT(nn.Module):\n",
            "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
            "\n",
            "    def __init__(self, config):\n",
            "        super().__init__()\n",
            "\n",
            "        # input embedding stem\n",
            "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
            "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
            "        self.drop = nn.Dropout(config.embd_pdrop)\n",
            "        # transformer\n",
            "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
            "        # decoder head\n",
            "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
            "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
            "\n",
            "        self.block_size = config.block_size\n",
            "        self.apply(self._init_weights)\n",
            "\n",
            "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
            "        # init weights\n",
            "        self.old_lm = forget.init()\n",
            "        self.lda1 = nn.Linear(config.block_size, config.word_num, bias=False)\n",
            "        self.lda2 = nn.Linear(config.block_size, config.word_num, bias=True)\n",
            "        self.vop_emb = nn.Variable(templat_config.vocab, trainable=trainable=trainable=False)\n",
            "        self.init_block(config.slice_size, config.word_num)\n",
            "\n",
            "        self.transformer = Transformer(config, config.sequence_length)\n",
            "        self.init_transformer(config, trans.init_config)\n",
            "\n",
            "    def init_emb(self, config):\n",
            "        self.tok_emb.normal_(0, 0.01)\n",
            "        self.pos_emb.zero_()\n",
            "        self.tok_emb.weight.data.copy_(config.temp_emb)\n",
            "        self.pos_emb.weight.data.copy_(config.pos_emb)\n",
            "        self.vop_emb = nn.Variable(templat_config.vocab, trainable=trainable=trainable=False)\n",
            "        self.init_block(config.slice_size, config.word_num)\n",
            "\n",
            "    def init_block(self, slice_size, word_num):\n",
            "        self.block = Dense(config.size_vocab, activation='elu', shape=(config.block_size, slice_size, word_num))\n",
            "\n",
            "        self.apply(Dense(config.word_num, activation='elu', shape=(config.block_size, slice_size, config.vocab_size)))\n",
            "        self.apply(Dense(config.size_vocab, activation='elu'))\n",
            "        self.apply(Dropout(config.embd_pdrop))\n",
            "\n",
            "    def apply(self, func):\n",
            "        self.store_in_param = self.aux2func(func)\n",
            "\n",
            "    def store_in_param(self, f, slice_idx, shape=None):\n",
            "        return self.aux(f, slice_idx, shape=shape, initialized=True)\n",
            "\n",
            "    def aux(self, func, slice_idx, shape=None, initialized=True):\n",
            "        # transform their output\n",
            "        orig_embeddings = self.transformer.transformer_arch.transformer_arch.embeddings\n",
            "\n",
            "        # dst_emb for the transformer\n",
            "        emb = self.tok_emb.weight\n",
            "        if initialized:\n",
            "            emb = trans.init_emb_utils(emb, orig_embeddings[:, 0], initialized=True)\n",
            "        elif shape is None and initialized:\n",
            "            emb = trans.init_emb_utils(emb, orig_embeddings[:, 0], shape=shape)\n",
            "        self.transformer.transformer_arch.transformer_arch.init_mse_wrapper(emb, orig_embeddings[:, 0])\n",
            "        # decoder embedding stem\n",
            "        self.pos_emb.weight.data.copy_(orig_embeddings[:, orig_embeddings.low()[:, slice_idx, :, :].view(-1, config.block_size, config.n_embd)])\n",
            "        # decoder head weight\n",
            "        self.ln_f.weight.data.copy_(orig_embeddings[:, orig_embeddings.low()[:, :, :].view(config.sub_embed_size, config.block_size, config.n_embd)])\n",
            "        # decoder head\n",
            "        self.ln_f.weight.data.copy_(orig_embeddings[:, orig_embeddings.low()[:, :, :].view(config.sub_embed_size, config.block_size, config.n_embd)])\n",
            "        # last weight\n",
            "        self.ln_f.weight.data.copy_(orig_embeddings[:, orig_embeddings.low()[:, :, :].view(config.sub_embed_size, config.block_size, config.n_embd)])\n",
            "\n",
            "        # use the last active weight\n",
            "        self.vop_emb.data.copy_(orig_embeddings[:, orig_embeddings.low()[:, :, :].view(-1, config.block_size, config.n_embd)])\n",
            "\n",
            "    def aux_post(self, f, slice_idx, shape=\"locs\"):\n",
            "        orig_embeddings = self.transformer.transformer_arch.transformer_arch.embeddings\n",
            "\n",
            "        # dst_emb for the transformer\n",
            "        emb, shape = f(self.transformer.transformer_arch.transformer_arch.embeddings[:, 0], slice_idx, shape)\n",
            "        if initialized:\n",
            "            emb = trans.init_emb_utils(emb, orig_embeddings[:, 0], initialized=True)\n",
            "        elif shape is None and initialized:\n",
            "            emb = trans.init_emb_utils(emb, orig_embeddings[:, 0], shape=shape)\n",
            "        self.transformer.transformer_arch.transformer_arch.init_mse_wrapper(emb, orig_embeddings[:, 0])\n",
            "        # decoder embedding stem\n",
            "        self.pos_emb.weight.data.copy_(orig_embeddings[:, orig_embeddings.low()[:, :, :].view(config.block_size, config.n_embd)])\n",
            "        # decoder head weight\n",
            "        self.ln_f.weight.data.copy_(orig_embeddings[:, orig_embeddings.low()[:, :, :].view(config.block_size, config.n_embd)])\n",
            "        # decoder head\n",
            "        self.ln_f.weight.data.copy_(\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Enqueue next (1) batch(es) of data to infeed.\n",
            "Dequeue next (1) batch(es) of data from outfeed.\n",
            "Outfeed finished for iteration (1, 0)\n",
            "Stop infeed thread controller\n",
            "Shutting down InfeedController thread.\n",
            "InfeedController received shutdown signal, stopping.\n",
            "Infeed thread finished, shutting down.\n",
            "infeed marked as finished\n",
            "Stop output thread controller\n",
            "Shutting down OutfeedController thread.\n",
            "OutfeedController received shutdown signal, stopping.\n",
            "Outfeed thread finished, shutting down.\n",
            "outfeed marked as finished\n",
            "Shutdown TPU system.\n",
            "prediction_loop marked as finished\n",
            "prediction_loop marked as finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3bh16K12pHK"
      },
      "source": [
        "**Evaluate the Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH4x18ve2yLO"
      },
      "source": [
        "**Wikitext Dataset Evaluation**\n",
        "\n",
        "\n",
        "1.   Download wikitext test set\n",
        "2.   Tokenize and upload to Google Cloud Storage\n",
        "3.   Create dataset config\n",
        "4.   Update model config to point to wikitext test set\n",
        "5.   Run the model in eval mode over tokenized data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urCdBQBa2tm4",
        "outputId": "332ffe7b-310a-4f6d-83f9-f957df03bfa3"
      },
      "source": [
        "wikitext103_src = \"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\"\n",
        "!wget $wikitext103_src\n",
        "!unzip wikitext-103-raw-v1.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-27 21:01:22--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.192.16\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.192.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191984949 (183M) [application/zip]\n",
            "Saving to: ‘wikitext-103-raw-v1.zip’\n",
            "\n",
            "wikitext-103-raw-v1 100%[===================>] 183.09M  72.6MB/s    in 2.5s    \n",
            "\n",
            "2021-03-27 21:01:25 (72.6 MB/s) - ‘wikitext-103-raw-v1.zip’ saved [191984949/191984949]\n",
            "\n",
            "Archive:  wikitext-103-raw-v1.zip\n",
            "   creating: wikitext-103-raw/\n",
            "  inflating: wikitext-103-raw/wiki.test.raw  \n",
            "  inflating: wikitext-103-raw/wiki.valid.raw  \n",
            "  inflating: wikitext-103-raw/wiki.train.raw  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXiG7WxT26B9",
        "outputId": "bfa4a5ea-76c9-4092-94ec-af7cf95bb2b3"
      },
      "source": [
        "\n",
        "!mkdir wikitext\n",
        "!mv /content/GPTNeo/wikitext-103-raw/wiki.test.raw wikitext/wikitext_test.txt\n",
        "\n",
        "# Tokenize Data\n",
        "!python data/create_tfrecords.py --input_dir wikitext --name wikitext --files_per 1000 --output_dir wikitext_tokenized --write_dataset_config --processes 1 --wikitext-detokenize\n",
        "\n",
        "# copy the data to your bucket\n",
        "if not path_to_cloud_bucket.endswith('/'):\n",
        "  path_to_cloud_bucket += '/'\n",
        "copy_loc = path_to_cloud_bucket \n",
        "!gsutil -m cp -r wikitext_tokenized $copy_loc\n",
        "!gsutil ls $path_to_cloud_bucket"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-27 21:02:17.757328: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Writing TFRecord Files to wikitext_tokenized/. Parsed 0 input files. files_written : 0it [00:02, ?it/s]\n",
            "{'discarded': 0, 'processed': 1, 'successful': 1}\n",
            "Copying file://wikitext_tokenized/wikitext_0_139.tfrecords [Content-Type=application/octet-stream]...\n",
            "/ [1/1 files][578.6 KiB/578.6 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/578.6 KiB.                                    \n",
            "gs://ditto_gptneo/GPT3_XL/\n",
            "gs://ditto_gptneo/datasets/\n",
            "gs://ditto_gptneo/wikitext_tokenized/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T17Im9ysVcpi",
        "outputId": "c7a33867-7453-4874-f8f0-f9e02a00a2da"
      },
      "source": [
        "%%writefile configs/dataset_configs/wikitext.json\n",
        "\n",
        "{\n",
        "  \"path\": \"\",\n",
        "  \"eval_path\": \"gs://ditto_gptneo/wikitext_tokenized/*.tfrecords\",\n",
        "  \"n_vocab\": 50256,\n",
        "  \"tokenizer_is_pretrained\": true,\n",
        "  \"tokenizer_path\": \"gpt2\",\n",
        "  \"eos_id\": 50256,\n",
        "  \"padding_id\": 50257\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting configs/dataset_configs/wikitext.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hD_6qO942My",
        "outputId": "7a6b06fc-5e2a-4766-a95f-0bfd476f48c4"
      },
      "source": [
        "# @title Modify config for wikitext. \n",
        "  \n",
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "batch_size = 8 #@param {type:\"integer\"}\n",
        "assert pretrained_model is not None\n",
        "with open(f'configs/{pretrained_model}.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "  pprint(data)\n",
        "  dset_val = [[\"wikitext\", None, None, None]]\n",
        "  mods = {\n",
        "          \"datasets\": dset_val,\n",
        "          \"eval_steps\": 139 // batch_size,\n",
        "          \"train_batch_size\": batch_size,\n",
        "          \"eval_batch_size\": batch_size,\n",
        "        }\n",
        "  data.update(mods)\n",
        "  print('\\n--->\\n')\n",
        "  pprint(data)\n",
        "  with open(f'{pretrained_model}.json', 'w') as outfile:\n",
        "    json.dump(data, outfile, indent=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'activation_function': 'gelu',\n",
            " 'attention_types': [[['global', 'local'], 12]],\n",
            " 'attn_dropout': 0,\n",
            " 'beta1': 0.9,\n",
            " 'beta2': 0.95,\n",
            " 'datasets': [['wikitext', None, None, None]],\n",
            " 'embed_dropout': 0,\n",
            " 'eos_id': 50256,\n",
            " 'epsilon': 1e-08,\n",
            " 'eval_batch_size': 8,\n",
            " 'eval_steps': 17,\n",
            " 'eval_tasks': '',\n",
            " 'gradient_clipping': 1.0,\n",
            " 'iterations': 500,\n",
            " 'layout': 'intermediate_expanded:x,heads:x,memory_length:y,embd:y',\n",
            " 'lr': 0.0002,\n",
            " 'lr_decay': 'cosine',\n",
            " 'lr_decay_end': 300000,\n",
            " 'mesh_shape': 'x:4,y:2',\n",
            " 'model_path': 'gs://ditto_gptneo/GPT3_XL',\n",
            " 'n_ctx': 2048,\n",
            " 'n_embd': 2048,\n",
            " 'n_head': 16,\n",
            " 'n_layer': 24,\n",
            " 'n_vocab': 50257,\n",
            " 'opt_name': 'adam',\n",
            " 'padding_id': 50257,\n",
            " 'precision': 'bfloat16',\n",
            " 'predict_batch_size': 8,\n",
            " 'predict_steps': 0,\n",
            " 'recompute_grad': True,\n",
            " 'res_dropout': 0,\n",
            " 'scale_by_depth': True,\n",
            " 'scale_by_in': False,\n",
            " 'tokens_per_mb_per_replica': 4096,\n",
            " 'train_batch_size': 8,\n",
            " 'train_steps': 363000,\n",
            " 'warmup_steps': 3000,\n",
            " 'weight_decay': 0}\n",
            "\n",
            "--->\n",
            "\n",
            "{'activation_function': 'gelu',\n",
            " 'attention_types': [[['global', 'local'], 12]],\n",
            " 'attn_dropout': 0,\n",
            " 'beta1': 0.9,\n",
            " 'beta2': 0.95,\n",
            " 'datasets': [['wikitext', None, None, None]],\n",
            " 'embed_dropout': 0,\n",
            " 'eos_id': 50256,\n",
            " 'epsilon': 1e-08,\n",
            " 'eval_batch_size': 8,\n",
            " 'eval_steps': 17,\n",
            " 'eval_tasks': '',\n",
            " 'gradient_clipping': 1.0,\n",
            " 'iterations': 500,\n",
            " 'layout': 'intermediate_expanded:x,heads:x,memory_length:y,embd:y',\n",
            " 'lr': 0.0002,\n",
            " 'lr_decay': 'cosine',\n",
            " 'lr_decay_end': 300000,\n",
            " 'mesh_shape': 'x:4,y:2',\n",
            " 'model_path': 'gs://ditto_gptneo/GPT3_XL',\n",
            " 'n_ctx': 2048,\n",
            " 'n_embd': 2048,\n",
            " 'n_head': 16,\n",
            " 'n_layer': 24,\n",
            " 'n_vocab': 50257,\n",
            " 'opt_name': 'adam',\n",
            " 'padding_id': 50257,\n",
            " 'precision': 'bfloat16',\n",
            " 'predict_batch_size': 8,\n",
            " 'predict_steps': 0,\n",
            " 'recompute_grad': True,\n",
            " 'res_dropout': 0,\n",
            " 'scale_by_depth': True,\n",
            " 'scale_by_in': False,\n",
            " 'tokens_per_mb_per_replica': 4096,\n",
            " 'train_batch_size': 8,\n",
            " 'train_steps': 363000,\n",
            " 'warmup_steps': 3000,\n",
            " 'weight_decay': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPK0zNFH47mt",
        "outputId": "d2071432-08bb-41e1-a9ef-6bbe74228235"
      },
      "source": [
        "!python3 main.py --eval --tpu colab --model $pretrained_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-28 00:58:50.526720: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "Current step 362000\n",
            "Saving config to gs://ditto_gptneo/GPT3_XL\n",
            "2021-03-28 00:58:56.167556: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-03-28 00:58:56.168650: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-03-28 00:58:56.179161: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-03-28 00:58:56.179214: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (b9f32c1e6fe8): /proc/driver/nvidia/version does not exist\n",
            "2021-03-28 00:58:56.750935: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
            "Done!\n",
            "params = defaultdict(<function fetch_model_params.<locals>.<lambda> at 0x7f17f1199560>, {'n_head': 16, 'n_vocab': 50257, 'embed_dropout': 0, 'lr': 0.0002, 'lr_decay': 'cosine', 'warmup_steps': 3000, 'beta1': 0.9, 'beta2': 0.95, 'epsilon': 1e-08, 'opt_name': 'adam', 'weight_decay': 0, 'train_batch_size': 8, 'attn_dropout': 0, 'train_steps': 363000, 'lr_decay_end': 300000, 'eval_steps': 17, 'predict_steps': 0, 'res_dropout': 0, 'eval_batch_size': 8, 'predict_batch_size': 8, 'iterations': 500, 'n_embd': 2048, 'datasets': [['wikitext', None, None, None]], 'model_path': 'gs://ditto_gptneo/GPT3_XL', 'n_ctx': 2048, 'n_layer': 24, 'scale_by_depth': True, 'scale_by_in': False, 'attention_types': ['global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local'], 'mesh_shape': 'x:4,y:2', 'layout': 'intermediate_expanded:x,heads:x,memory_length:y,embd:y', 'activation_function': 'gelu', 'recompute_grad': True, 'gradient_clipping': 1.0, 'tokens_per_mb_per_replica': 4096, 'precision': 'bfloat16', 'padding_id': 50257, 'eos_id': 50256, 'eval_tasks': '', 'dataset_configs': {'wikitext': {'path': '', 'eval_path': 'gs://ditto_gptneo/wikitext_tokenized/*.tfrecords', 'n_vocab': 50256, 'tokenizer_is_pretrained': True, 'tokenizer_path': 'gpt2', 'eos_id': 50256, 'padding_id': 50257}}, 'mlm_training': False, 'causal': True, 'num_cores': 8, 'auto_layout': False, 'auto_layout_and_mesh_shape': False, 'use_tpu': True, 'gpu_ids': ['device:GPU:0'], 'steps_per_checkpoint': 5000, 'predict': False, 'model': 'GPT', 'export': False, 'sampling_use_entmax': False, 'moe_layers': None, 'slow_sampling': False})\n",
            "Using config: {'_model_dir': 'gs://ditto_gptneo/GPT3_XL', '_tf_random_seed': None, '_save_summary_steps': 500, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.40.85.146:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.40.85.146:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.40.85.146:8470', '_evaluation_master': 'grpc://10.40.85.146:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=500, num_shards=8, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7f17f1258bd0>}\n",
            "_TPUContext: eval_on_tpu True\n",
            "Running evaluation...\n",
            "Querying Tensorflow master (grpc://10.40.85.146:8470) for TPU system metadata.\n",
            "2021-03-28 00:58:57.001005: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "Initializing TPU system (master: grpc://10.40.85.146:8470) to fetch topology for model parallelism. This might take a while.\n",
            "Found TPU system:\n",
            "*** Num TPU Cores: 8\n",
            "*** Num TPU Workers: 1\n",
            "*** Num TPU Cores Per Worker: 8\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 6249188509115223616)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -4146733154084846229)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 5892924184187693777)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 2462857471687540533)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5469279350878100045)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, -9117572227669529228)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -3216840776060473709)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, -9064804662247500787)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -2631494962670085392)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 1762238546668444786)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 1041094745256039506)\n",
            "Calling model_fn.\n",
            "WARNING:root:Changing batch size with sequential_input() will result in some data being skipped or repeated. Please ensure your batch size stays constant throughout training.\n",
            "num_cores_per_replica: 1\n",
            "computation_shape: [1, 1, 1, 1]\n",
            "num_replicas: 8\n",
            "device_assignment.topology.device_coordinates: [[[0 0 0 0]\n",
            "  [0 0 0 1]\n",
            "  [1 0 0 0]\n",
            "  [1 0 0 1]\n",
            "  [0 1 0 0]\n",
            "  [0 1 0 1]\n",
            "  [1 1 0 0]\n",
            "  [1 1 0 1]]]\n",
            "device_assignment.core_assignment: [[[0 0 0 0]]\n",
            "\n",
            " [[0 0 0 1]]\n",
            "\n",
            " [[1 0 0 0]]\n",
            "\n",
            " [[1 0 0 1]]\n",
            "\n",
            " [[0 1 0 0]]\n",
            "\n",
            " [[0 1 0 1]]\n",
            "\n",
            " [[1 1 0 0]]\n",
            "\n",
            " [[1 1 0 1]]]\n",
            "2021-03-28 00:59:08.989605: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "device_list = ['/job:worker/task:0/device:CPU:0']\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "SimdMeshImpl init: Shape[x=4, y=2] LayoutRules{('embd', 'y'), ('memory_length', 'y'), ('heads', 'x'), ('intermediate_expanded', 'x')}\n",
            "Device Assignment: <tensorflow.python.tpu.device_assignment.DeviceAssignment object at 0x7f17eb37e190>\n",
            "\n",
            "\n",
            "N TRAINABLE VARS:\n",
            "1,315,575,808\n",
            "\n",
            "\n",
            "ALL DIM NAMES:\n",
            "vocab\n",
            "embd\n",
            "intermediate_expanded\n",
            "heads\n",
            "embed_sequence\n",
            "\n",
            "\n",
            "Create pnum_tensor\n",
            "Variable gpt2/h0/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h0/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h0/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h0/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h0/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h0/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h1/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h1/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h1/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h1/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h1/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h1/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h10/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h10/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h10/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h10/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h10/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h10/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h11/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h11/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h11/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h11/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h11/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h11/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h12/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h12/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h12/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h12/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h12/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h12/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h13/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h13/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h13/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h13/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h13/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h13/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h14/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h14/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h14/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h14/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h14/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h14/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h15/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h15/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h15/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h15/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h15/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h15/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h16/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h16/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h16/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h16/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h16/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h16/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h17/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h17/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h17/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h17/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h17/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h17/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h18/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h18/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h18/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h18/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h18/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h18/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h19/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h19/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h19/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h19/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h19/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h19/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h2/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h2/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h2/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h2/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h2/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h2/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h20/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h20/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h20/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h20/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h20/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h20/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h21/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h21/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h21/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h21/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h21/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h21/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h22/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h22/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h22/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h22/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h22/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h22/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h23/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h23/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h23/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h23/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h23/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h23/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h3/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h3/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h3/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h3/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h3/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h3/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h4/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h4/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h4/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h4/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h4/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h4/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h5/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h5/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h5/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h5/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h5/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h5/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h6/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h6/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h6/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h6/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h6/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h6/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h7/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h7/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h7/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h7/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h7/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h7/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h8/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h8/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h8/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h8/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h8/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h8/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h9/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h9/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h9/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h9/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h9/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h9/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/wpe                                                     size 4194304      slice_size 2097152      Shape[embed_sequence=2048, embd=2048]                       \n",
            "Variable gpt2/wte                                                     size 102926336    slice_size 51463168     Shape[vocab=50257, embd=2048]                               \n",
            "Variable stacked/gpt2/h0/mlp/conv1d_main/c_fc/bias                    size 196608       slice_size 49152        Shape[stacked=24, intermediate_expanded=8192]               \n",
            "    gpt2/h0/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h1/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h2/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h3/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h4/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h5/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h6/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h7/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h8/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h9/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h10/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h11/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h12/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h13/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h14/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h15/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h16/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h17/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h18/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h19/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h20/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h21/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h22/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h23/mlp/conv1d_main/c_fc/bias\n",
            "Variable stacked/gpt2/h0/norm_1/g                                     size 131072       slice_size 65536        Shape[stacked=64, embd=2048]                                \n",
            "    gpt2/h0/norm_1/g\n",
            "    gpt2/h0/norm_1/b\n",
            "    gpt2/h0/attn/compute_output_bias/o_b\n",
            "    gpt2/h0/norm_2/g\n",
            "    gpt2/h0/norm_2/b\n",
            "    gpt2/h0/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h1/norm_1/g\n",
            "    gpt2/h1/norm_1/b\n",
            "    gpt2/h1/attn/compute_output_bias/o_b\n",
            "    gpt2/h1/norm_2/g\n",
            "    gpt2/h1/norm_2/b\n",
            "    gpt2/h1/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h2/norm_1/g\n",
            "    gpt2/h2/norm_1/b\n",
            "    gpt2/h2/attn/compute_output_bias/o_b\n",
            "    gpt2/h2/norm_2/g\n",
            "    gpt2/h2/norm_2/b\n",
            "    gpt2/h2/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h3/norm_1/g\n",
            "    gpt2/h3/norm_1/b\n",
            "    gpt2/h3/attn/compute_output_bias/o_b\n",
            "    gpt2/h3/norm_2/g\n",
            "    gpt2/h3/norm_2/b\n",
            "    gpt2/h3/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h4/norm_1/g\n",
            "    gpt2/h4/norm_1/b\n",
            "    gpt2/h4/attn/compute_output_bias/o_b\n",
            "    gpt2/h4/norm_2/g\n",
            "    gpt2/h4/norm_2/b\n",
            "    gpt2/h4/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h5/norm_1/g\n",
            "    gpt2/h5/norm_1/b\n",
            "    gpt2/h5/attn/compute_output_bias/o_b\n",
            "    gpt2/h5/norm_2/g\n",
            "    gpt2/h5/norm_2/b\n",
            "    gpt2/h5/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h6/norm_1/g\n",
            "    gpt2/h6/norm_1/b\n",
            "    gpt2/h6/attn/compute_output_bias/o_b\n",
            "    gpt2/h6/norm_2/g\n",
            "    gpt2/h6/norm_2/b\n",
            "    gpt2/h6/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h7/norm_1/g\n",
            "    gpt2/h7/norm_1/b\n",
            "    gpt2/h7/attn/compute_output_bias/o_b\n",
            "    gpt2/h7/norm_2/g\n",
            "    gpt2/h7/norm_2/b\n",
            "    gpt2/h7/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h8/norm_1/g\n",
            "    gpt2/h8/norm_1/b\n",
            "    gpt2/h8/attn/compute_output_bias/o_b\n",
            "    gpt2/h8/norm_2/g\n",
            "    gpt2/h8/norm_2/b\n",
            "    gpt2/h8/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h9/norm_1/g\n",
            "    gpt2/h9/norm_1/b\n",
            "    gpt2/h9/attn/compute_output_bias/o_b\n",
            "    gpt2/h9/norm_2/g\n",
            "    gpt2/h9/norm_2/b\n",
            "    gpt2/h9/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h10/norm_1/g\n",
            "    gpt2/h10/norm_1/b\n",
            "    gpt2/h10/attn/compute_output_bias/o_b\n",
            "    gpt2/h10/norm_2/g\n",
            "Variable stacked/gpt2/h10/norm_2/b                                    size 131072       slice_size 65536        Shape[stacked=64, embd=2048]                                \n",
            "    gpt2/h10/norm_2/b\n",
            "    gpt2/h10/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h11/norm_1/g\n",
            "    gpt2/h11/norm_1/b\n",
            "    gpt2/h11/attn/compute_output_bias/o_b\n",
            "    gpt2/h11/norm_2/g\n",
            "    gpt2/h11/norm_2/b\n",
            "    gpt2/h11/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h12/norm_1/g\n",
            "    gpt2/h12/norm_1/b\n",
            "    gpt2/h12/attn/compute_output_bias/o_b\n",
            "    gpt2/h12/norm_2/g\n",
            "    gpt2/h12/norm_2/b\n",
            "    gpt2/h12/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h13/norm_1/g\n",
            "    gpt2/h13/norm_1/b\n",
            "    gpt2/h13/attn/compute_output_bias/o_b\n",
            "    gpt2/h13/norm_2/g\n",
            "    gpt2/h13/norm_2/b\n",
            "    gpt2/h13/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h14/norm_1/g\n",
            "    gpt2/h14/norm_1/b\n",
            "    gpt2/h14/attn/compute_output_bias/o_b\n",
            "    gpt2/h14/norm_2/g\n",
            "    gpt2/h14/norm_2/b\n",
            "    gpt2/h14/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h15/norm_1/g\n",
            "    gpt2/h15/norm_1/b\n",
            "    gpt2/h15/attn/compute_output_bias/o_b\n",
            "    gpt2/h15/norm_2/g\n",
            "    gpt2/h15/norm_2/b\n",
            "    gpt2/h15/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h16/norm_1/g\n",
            "    gpt2/h16/norm_1/b\n",
            "    gpt2/h16/attn/compute_output_bias/o_b\n",
            "    gpt2/h16/norm_2/g\n",
            "    gpt2/h16/norm_2/b\n",
            "    gpt2/h16/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h17/norm_1/g\n",
            "    gpt2/h17/norm_1/b\n",
            "    gpt2/h17/attn/compute_output_bias/o_b\n",
            "    gpt2/h17/norm_2/g\n",
            "    gpt2/h17/norm_2/b\n",
            "    gpt2/h17/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h18/norm_1/g\n",
            "    gpt2/h18/norm_1/b\n",
            "    gpt2/h18/attn/compute_output_bias/o_b\n",
            "    gpt2/h18/norm_2/g\n",
            "    gpt2/h18/norm_2/b\n",
            "    gpt2/h18/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h19/norm_1/g\n",
            "    gpt2/h19/norm_1/b\n",
            "    gpt2/h19/attn/compute_output_bias/o_b\n",
            "    gpt2/h19/norm_2/g\n",
            "    gpt2/h19/norm_2/b\n",
            "    gpt2/h19/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h20/norm_1/g\n",
            "    gpt2/h20/norm_1/b\n",
            "    gpt2/h20/attn/compute_output_bias/o_b\n",
            "    gpt2/h20/norm_2/g\n",
            "    gpt2/h20/norm_2/b\n",
            "    gpt2/h20/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h21/norm_1/g\n",
            "    gpt2/h21/norm_1/b\n",
            "Variable stacked/gpt2/h21/attn/compute_output_bias/o_b                size 36864        slice_size 18432        Shape[stacked=18, embd=2048]                                \n",
            "    gpt2/h21/attn/compute_output_bias/o_b\n",
            "    gpt2/h21/norm_2/g\n",
            "    gpt2/h21/norm_2/b\n",
            "    gpt2/h21/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h22/norm_1/g\n",
            "    gpt2/h22/norm_1/b\n",
            "    gpt2/h22/attn/compute_output_bias/o_b\n",
            "    gpt2/h22/norm_2/g\n",
            "    gpt2/h22/norm_2/b\n",
            "    gpt2/h22/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h23/norm_1/g\n",
            "    gpt2/h23/norm_1/b\n",
            "    gpt2/h23/attn/compute_output_bias/o_b\n",
            "    gpt2/h23/norm_2/g\n",
            "    gpt2/h23/norm_2/b\n",
            "    gpt2/h23/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/ln_f/g\n",
            "    gpt2/ln_f/b\n",
            "Trainable Variables            count: 150     Total size: 1315575808       Total slice_size: 204753920      \n",
            "All Variables                  count: 150     Total size: 1315575808       Total slice_size: 204753920      \n",
            "Counters:\n",
            "allreduce: 2.51e+10\n",
            " allreduce/[0]: 6.44e+09\n",
            "  allreduce/[0]/einsum_op: 6.44e+09\n",
            " allreduce/[1]: 1.87e+10\n",
            "  allreduce/[1]/einsum_op: 1.87e+10\n",
            "  allreduce/[1]/reduce_op: 2.54e+07\n",
            "einsum: 3.58e+13\n",
            "einsum_unique: 2.52e+13\n",
            "output: 3.14e+11\n",
            " output/AddOperation: 8.13e+10\n",
            " output/BinaryOpWithBroadcasting: 5.2e+08\n",
            " output/BroadcastOperation: 6.48e+09\n",
            " output/ConcatOperation: 3.22e+09\n",
            " output/Constant: 1.97e+05\n",
            " output/EinsumOperation: 7.64e+10\n",
            " output/ImportOperation: 2.62e+05\n",
            " output/OneHotOperation: 1.32e+10\n",
            " output/RangeOperation: 2.38e+05\n",
            " output/ReduceOperation: 3.85e+07\n",
            " output/ReshapeOperation: 1.21e+10\n",
            " output/ScalarAddOperation: 6.45e+09\n",
            " output/ScalarMultiplyOperation: 2.27e+10\n",
            " output/ShiftOperation: 1.61e+09\n",
            " output/SlicewiseOperation: 7.22e+10\n",
            " output/StackedVariable: 1.59e+06\n",
            " output/StopGradient: 1.63e+10\n",
            " output/TopKOperation: 2.62e+05\n",
            " output/UnstackOperation: 1.59e+06\n",
            " output/Variable: 1.64e+09\n",
            "output_unique: 1.51e+11\n",
            " output_unique/AddOperation: 3.88e+10\n",
            " output_unique/BinaryOpWithBroadcasting: 6.71e+07\n",
            " output_unique/BroadcastOperation: 6.45e+09\n",
            " output_unique/ConcatOperation: 1.61e+09\n",
            " output_unique/Constant: 2.46e+04\n",
            " output_unique/EinsumOperation: 3.15e+10\n",
            " output_unique/ImportOperation: 3.28e+04\n",
            " output_unique/OneHotOperation: 1.65e+09\n",
            " output_unique/RangeOperation: 3.07e+04\n",
            " output_unique/ReduceOperation: 1.43e+07\n",
            " output_unique/ReshapeOperation: 6.44e+09\n",
            " output_unique/ScalarAddOperation: 3.22e+09\n",
            " output_unique/ScalarMultiplyOperation: 1.05e+10\n",
            " output_unique/ShiftOperation: 8.05e+08\n",
            " output_unique/SlicewiseOperation: 4e+10\n",
            " output_unique/StackedVariable: 4.96e+05\n",
            " output_unique/StopGradient: 8.88e+09\n",
            " output_unique/TopKOperation: 3.28e+04\n",
            " output_unique/UnstackOperation: 4.96e+05\n",
            " output_unique/Variable: 1.32e+09\n",
            "variables: 1.32e+09\n",
            " variables/trainable: 1.32e+09\n",
            "From /content/GPTNeo/model_fns.py:235: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3423: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Done calling model_fn.\n",
            "Starting evaluation at 2021-03-28T00:59:25Z\n",
            "TPU job name worker\n",
            "Graph was finalized.\n",
            "Restoring parameters from gs://ditto_gptneo/GPT3_XL/model.ckpt-362000\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:824: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "Starting infeed thread controller.\n",
            "Starting outfeed thread controller.\n",
            "Initialized dataset iterators in 0 seconds\n",
            "Before copy master to slices.\n",
            "Done with copy master to slices.\n",
            "Enqueue next (17) batch(es) of data to infeed.\n",
            "Dequeue next (17) batch(es) of data from outfeed.\n",
            "Outfeed finished for iteration (0, 0)\n",
            "Evaluation [17/17]\n",
            "Stop infeed thread controller\n",
            "Shutting down InfeedController thread.\n",
            "InfeedController received shutdown signal, stopping.\n",
            "Infeed thread finished, shutting down.\n",
            "infeed marked as finished\n",
            "Stop output thread controller\n",
            "Shutting down OutfeedController thread.\n",
            "OutfeedController received shutdown signal, stopping.\n",
            "Outfeed thread finished, shutting down.\n",
            "outfeed marked as finished\n",
            "Shutdown TPU system.\n",
            "Inference Time : 27.69739s\n",
            "Finished evaluation at 2021-03-28-00:59:53\n",
            "Saving dict for global step 362000: bits per byte = 1.0828223, global_step = 362000, loss = 2.5585654, mean_logits = -16.713398, perplexity = 13.107158\n",
            "Saving 'checkpoint_path' summary for global step 362000: gs://ditto_gptneo/GPT3_XL/model.ckpt-362000\n",
            "evaluation_loop marked as finished\n",
            "Eval results: {'bits per byte': 1.0828223, 'loss': 2.5585654, 'mean_logits': -16.713398, 'perplexity': 13.107158, 'global_step': 362000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO0x67tm5iOL"
      },
      "source": [
        "**Lambada**\n",
        "\n",
        "Modify config for Lambada and Run Lambada eval\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq18b2I35ktH",
        "outputId": "90f3e9e1-b9a8-45ec-ddcd-20fd65601c43"
      },
      "source": [
        "# @title Modify config for Lambada. \n",
        "  \n",
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "batch_size = 8 #@param {type:\"integer\"}\n",
        "assert pretrained_model is not None\n",
        "with open(f'configs/{pretrained_model}.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "  mods = {\n",
        "          \"datasets\": dset_val,\n",
        "          \"eval_steps\": 0,\n",
        "          \"train_batch_size\": batch_size,\n",
        "          \"eval_batch_size\": batch_size,\n",
        "          \"eval_tasks\": [\"lambada\"]\n",
        "        }\n",
        "  data.update(mods)\n",
        "  print('\\n--->\\n')\n",
        "  pprint(data)\n",
        "  with open(f'configs/{pretrained_model}.json', 'w') as outfile:\n",
        "    json.dump(data, outfile, indent=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--->\n",
            "\n",
            "{'activation_function': 'gelu',\n",
            " 'attention_types': [[['global', 'local'], 12]],\n",
            " 'attn_dropout': 0,\n",
            " 'beta1': 0.9,\n",
            " 'beta2': 0.95,\n",
            " 'datasets': [['wikitext', None, None, None]],\n",
            " 'embed_dropout': 0,\n",
            " 'eos_id': 50256,\n",
            " 'epsilon': 1e-08,\n",
            " 'eval_batch_size': 8,\n",
            " 'eval_steps': 0,\n",
            " 'eval_tasks': ['lambada'],\n",
            " 'gradient_clipping': 1.0,\n",
            " 'iterations': 500,\n",
            " 'layout': 'intermediate_expanded:x,heads:x,memory_length:y,embd:y',\n",
            " 'lr': 0.0002,\n",
            " 'lr_decay': 'cosine',\n",
            " 'lr_decay_end': 300000,\n",
            " 'mesh_shape': 'x:4,y:2',\n",
            " 'model_path': 'gs://ditto_gptneo/GPT3_XL',\n",
            " 'n_ctx': 2048,\n",
            " 'n_embd': 2048,\n",
            " 'n_head': 16,\n",
            " 'n_layer': 24,\n",
            " 'n_vocab': 50257,\n",
            " 'opt_name': 'adam',\n",
            " 'padding_id': 50257,\n",
            " 'precision': 'bfloat16',\n",
            " 'predict_batch_size': 8,\n",
            " 'predict_steps': 0,\n",
            " 'recompute_grad': True,\n",
            " 'res_dropout': 0,\n",
            " 'scale_by_depth': True,\n",
            " 'scale_by_in': False,\n",
            " 'tokens_per_mb_per_replica': 4096,\n",
            " 'train_batch_size': 8,\n",
            " 'train_steps': 363000,\n",
            " 'warmup_steps': 3000,\n",
            " 'weight_decay': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c44B14Fh5m9R",
        "outputId": "43ed65b5-3fef-4440-c721-5885102614ea"
      },
      "source": [
        "!python3 main.py --eval --tpu colab --model $pretrained_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-28 01:02:41.710327: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "Current step 362000\n",
            "Saving config to gs://ditto_gptneo/GPT3_XL\n",
            "2021-03-28 01:02:47.309985: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-03-28 01:02:47.311196: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-03-28 01:02:47.322203: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-03-28 01:02:47.322266: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (b9f32c1e6fe8): /proc/driver/nvidia/version does not exist\n",
            "2021-03-28 01:02:47.916609: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
            "Done!\n",
            "params = defaultdict(<function fetch_model_params.<locals>.<lambda> at 0x7f0027247560>, {'n_head': 16, 'n_vocab': 50257, 'embed_dropout': 0, 'lr': 0.0002, 'lr_decay': 'cosine', 'warmup_steps': 3000, 'beta1': 0.9, 'beta2': 0.95, 'epsilon': 1e-08, 'opt_name': 'adam', 'weight_decay': 0, 'train_batch_size': 8, 'attn_dropout': 0, 'train_steps': 363000, 'lr_decay_end': 300000, 'eval_steps': 0, 'predict_steps': 0, 'res_dropout': 0, 'eval_batch_size': 8, 'predict_batch_size': 8, 'iterations': 500, 'n_embd': 2048, 'datasets': [['wikitext', None, None, None]], 'model_path': 'gs://ditto_gptneo/GPT3_XL', 'n_ctx': 2048, 'n_layer': 24, 'scale_by_depth': True, 'scale_by_in': False, 'attention_types': ['global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local'], 'mesh_shape': 'x:4,y:2', 'layout': 'intermediate_expanded:x,heads:x,memory_length:y,embd:y', 'activation_function': 'gelu', 'recompute_grad': True, 'gradient_clipping': 1.0, 'tokens_per_mb_per_replica': 4096, 'precision': 'bfloat16', 'padding_id': 50257, 'eos_id': 50256, 'eval_tasks': ['lambada'], 'dataset_configs': {'wikitext': {'path': '', 'eval_path': 'gs://ditto_gptneo/wikitext_tokenized/*.tfrecords', 'n_vocab': 50256, 'tokenizer_is_pretrained': True, 'tokenizer_path': 'gpt2', 'eos_id': 50256, 'padding_id': 50257}}, 'mlm_training': False, 'causal': True, 'num_cores': 8, 'auto_layout': False, 'auto_layout_and_mesh_shape': False, 'use_tpu': True, 'gpu_ids': ['device:GPU:0'], 'steps_per_checkpoint': 5000, 'predict': False, 'model': 'GPT', 'export': False, 'sampling_use_entmax': False, 'moe_layers': None, 'slow_sampling': False})\n",
            "Using config: {'_model_dir': 'gs://ditto_gptneo/GPT3_XL', '_tf_random_seed': None, '_save_summary_steps': 500, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.40.85.146:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.40.85.146:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.40.85.146:8470', '_evaluation_master': 'grpc://10.40.85.146:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=500, num_shards=8, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7f0027308cd0>}\n",
            "_TPUContext: eval_on_tpu True\n",
            "Using config: {'_model_dir': 'gs://ditto_gptneo/GPT3_XL', '_tf_random_seed': None, '_save_summary_steps': 500, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.40.85.146:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.40.85.146:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.40.85.146:8470', '_evaluation_master': 'grpc://10.40.85.146:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=500, num_shards=8, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7f0027308cd0>}\n",
            "_TPUContext: eval_on_tpu True\n",
            "Starting evaluation task 'lambada'\n",
            "Querying Tensorflow master (grpc://10.40.85.146:8470) for TPU system metadata.\n",
            "2021-03-28 01:02:48.277628: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "Initializing TPU system (master: grpc://10.40.85.146:8470) to fetch topology for model parallelism. This might take a while.\n",
            "Found TPU system:\n",
            "*** Num TPU Cores: 8\n",
            "*** Num TPU Workers: 1\n",
            "*** Num TPU Cores Per Worker: 8\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 6249188509115223616)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -4146733154084846229)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 5892924184187693777)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 2462857471687540533)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5469279350878100045)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, -9117572227669529228)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -3216840776060473709)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, -9064804662247500787)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -2631494962670085392)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 1762238546668444786)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 1041094745256039506)\n",
            "Calling model_fn.\n",
            "num_cores_per_replica: 1\n",
            "computation_shape: [1, 1, 1, 1]\n",
            "num_replicas: 8\n",
            "device_assignment.topology.device_coordinates: [[[0 0 0 0]\n",
            "  [0 0 0 1]\n",
            "  [1 0 0 0]\n",
            "  [1 0 0 1]\n",
            "  [0 1 0 0]\n",
            "  [0 1 0 1]\n",
            "  [1 1 0 0]\n",
            "  [1 1 0 1]]]\n",
            "device_assignment.core_assignment: [[[0 0 0 0]]\n",
            "\n",
            " [[0 0 0 1]]\n",
            "\n",
            " [[1 0 0 0]]\n",
            "\n",
            " [[1 0 0 1]]\n",
            "\n",
            " [[0 1 0 0]]\n",
            "\n",
            " [[0 1 0 1]]\n",
            "\n",
            " [[1 1 0 0]]\n",
            "\n",
            " [[1 1 0 1]]]\n",
            "2021-03-28 01:03:00.440841: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "device_list = ['/job:worker/task:0/device:CPU:0']\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "SimdMeshImpl init: Shape[x=4, y=2] LayoutRules{('intermediate_expanded', 'x'), ('heads', 'x'), ('memory_length', 'y'), ('embd', 'y')}\n",
            "Device Assignment: <tensorflow.python.tpu.device_assignment.DeviceAssignment object at 0x7f001f8a2310>\n",
            "\n",
            "\n",
            "N TRAINABLE VARS:\n",
            "1,315,575,808\n",
            "\n",
            "\n",
            "ALL DIM NAMES:\n",
            "intermediate_expanded\n",
            "embed_sequence\n",
            "heads\n",
            "vocab\n",
            "embd\n",
            "\n",
            "\n",
            "Create pnum_tensor\n",
            "Variable gpt2/h0/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h0/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h0/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h0/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h0/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h0/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h1/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h1/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h1/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h1/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h1/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h1/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h10/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h10/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h10/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h10/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h10/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h10/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h11/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h11/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h11/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h11/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h11/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h11/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h12/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h12/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h12/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h12/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h12/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h12/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h13/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h13/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h13/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h13/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h13/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h13/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h14/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h14/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h14/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h14/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h14/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h14/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h15/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h15/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h15/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h15/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h15/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h15/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h16/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h16/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h16/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h16/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h16/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h16/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h17/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h17/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h17/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h17/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h17/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h17/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h18/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h18/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h18/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h18/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h18/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h18/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h19/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h19/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h19/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h19/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h19/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h19/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h2/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h2/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h2/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h2/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h2/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h2/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h20/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h20/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h20/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h20/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h20/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h20/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h21/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h21/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h21/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h21/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h21/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h21/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h22/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h22/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h22/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h22/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h22/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h22/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h23/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h23/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h23/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h23/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h23/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h23/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h3/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h3/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h3/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h3/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h3/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h3/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h4/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h4/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h4/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h4/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h4/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h4/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h5/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h5/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h5/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h5/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h5/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h5/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h6/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h6/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h6/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h6/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h6/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h6/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h7/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h7/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h7/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h7/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h7/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h7/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h8/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h8/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h8/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h8/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h8/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h8/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h9/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h9/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h9/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h9/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h9/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h9/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/wpe                                                     size 4194304      slice_size 2097152      Shape[embed_sequence=2048, embd=2048]                       \n",
            "Variable gpt2/wte                                                     size 102926336    slice_size 51463168     Shape[vocab=50257, embd=2048]                               \n",
            "Variable stacked/gpt2/h0/mlp/conv1d_main/c_fc/bias                    size 196608       slice_size 49152        Shape[stacked=24, intermediate_expanded=8192]               \n",
            "    gpt2/h0/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h1/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h2/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h3/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h4/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h5/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h6/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h7/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h8/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h9/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h10/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h11/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h12/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h13/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h14/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h15/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h16/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h17/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h18/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h19/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h20/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h21/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h22/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h23/mlp/conv1d_main/c_fc/bias\n",
            "Variable stacked/gpt2/h0/norm_1/g                                     size 131072       slice_size 65536        Shape[stacked=64, embd=2048]                                \n",
            "    gpt2/h0/norm_1/g\n",
            "    gpt2/h0/norm_1/b\n",
            "    gpt2/h0/attn/compute_output_bias/o_b\n",
            "    gpt2/h0/norm_2/g\n",
            "    gpt2/h0/norm_2/b\n",
            "    gpt2/h0/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h1/norm_1/g\n",
            "    gpt2/h1/norm_1/b\n",
            "    gpt2/h1/attn/compute_output_bias/o_b\n",
            "    gpt2/h1/norm_2/g\n",
            "    gpt2/h1/norm_2/b\n",
            "    gpt2/h1/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h2/norm_1/g\n",
            "    gpt2/h2/norm_1/b\n",
            "    gpt2/h2/attn/compute_output_bias/o_b\n",
            "    gpt2/h2/norm_2/g\n",
            "    gpt2/h2/norm_2/b\n",
            "    gpt2/h2/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h3/norm_1/g\n",
            "    gpt2/h3/norm_1/b\n",
            "    gpt2/h3/attn/compute_output_bias/o_b\n",
            "    gpt2/h3/norm_2/g\n",
            "    gpt2/h3/norm_2/b\n",
            "    gpt2/h3/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h4/norm_1/g\n",
            "    gpt2/h4/norm_1/b\n",
            "    gpt2/h4/attn/compute_output_bias/o_b\n",
            "    gpt2/h4/norm_2/g\n",
            "    gpt2/h4/norm_2/b\n",
            "    gpt2/h4/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h5/norm_1/g\n",
            "    gpt2/h5/norm_1/b\n",
            "    gpt2/h5/attn/compute_output_bias/o_b\n",
            "    gpt2/h5/norm_2/g\n",
            "    gpt2/h5/norm_2/b\n",
            "    gpt2/h5/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h6/norm_1/g\n",
            "    gpt2/h6/norm_1/b\n",
            "    gpt2/h6/attn/compute_output_bias/o_b\n",
            "    gpt2/h6/norm_2/g\n",
            "    gpt2/h6/norm_2/b\n",
            "    gpt2/h6/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h7/norm_1/g\n",
            "    gpt2/h7/norm_1/b\n",
            "    gpt2/h7/attn/compute_output_bias/o_b\n",
            "    gpt2/h7/norm_2/g\n",
            "    gpt2/h7/norm_2/b\n",
            "    gpt2/h7/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h8/norm_1/g\n",
            "    gpt2/h8/norm_1/b\n",
            "    gpt2/h8/attn/compute_output_bias/o_b\n",
            "    gpt2/h8/norm_2/g\n",
            "    gpt2/h8/norm_2/b\n",
            "    gpt2/h8/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h9/norm_1/g\n",
            "    gpt2/h9/norm_1/b\n",
            "    gpt2/h9/attn/compute_output_bias/o_b\n",
            "    gpt2/h9/norm_2/g\n",
            "    gpt2/h9/norm_2/b\n",
            "    gpt2/h9/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h10/norm_1/g\n",
            "    gpt2/h10/norm_1/b\n",
            "    gpt2/h10/attn/compute_output_bias/o_b\n",
            "    gpt2/h10/norm_2/g\n",
            "Variable stacked/gpt2/h10/norm_2/b                                    size 131072       slice_size 65536        Shape[stacked=64, embd=2048]                                \n",
            "    gpt2/h10/norm_2/b\n",
            "    gpt2/h10/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h11/norm_1/g\n",
            "    gpt2/h11/norm_1/b\n",
            "    gpt2/h11/attn/compute_output_bias/o_b\n",
            "    gpt2/h11/norm_2/g\n",
            "    gpt2/h11/norm_2/b\n",
            "    gpt2/h11/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h12/norm_1/g\n",
            "    gpt2/h12/norm_1/b\n",
            "    gpt2/h12/attn/compute_output_bias/o_b\n",
            "    gpt2/h12/norm_2/g\n",
            "    gpt2/h12/norm_2/b\n",
            "    gpt2/h12/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h13/norm_1/g\n",
            "    gpt2/h13/norm_1/b\n",
            "    gpt2/h13/attn/compute_output_bias/o_b\n",
            "    gpt2/h13/norm_2/g\n",
            "    gpt2/h13/norm_2/b\n",
            "    gpt2/h13/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h14/norm_1/g\n",
            "    gpt2/h14/norm_1/b\n",
            "    gpt2/h14/attn/compute_output_bias/o_b\n",
            "    gpt2/h14/norm_2/g\n",
            "    gpt2/h14/norm_2/b\n",
            "    gpt2/h14/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h15/norm_1/g\n",
            "    gpt2/h15/norm_1/b\n",
            "    gpt2/h15/attn/compute_output_bias/o_b\n",
            "    gpt2/h15/norm_2/g\n",
            "    gpt2/h15/norm_2/b\n",
            "    gpt2/h15/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h16/norm_1/g\n",
            "    gpt2/h16/norm_1/b\n",
            "    gpt2/h16/attn/compute_output_bias/o_b\n",
            "    gpt2/h16/norm_2/g\n",
            "    gpt2/h16/norm_2/b\n",
            "    gpt2/h16/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h17/norm_1/g\n",
            "    gpt2/h17/norm_1/b\n",
            "    gpt2/h17/attn/compute_output_bias/o_b\n",
            "    gpt2/h17/norm_2/g\n",
            "    gpt2/h17/norm_2/b\n",
            "    gpt2/h17/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h18/norm_1/g\n",
            "    gpt2/h18/norm_1/b\n",
            "    gpt2/h18/attn/compute_output_bias/o_b\n",
            "    gpt2/h18/norm_2/g\n",
            "    gpt2/h18/norm_2/b\n",
            "    gpt2/h18/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h19/norm_1/g\n",
            "    gpt2/h19/norm_1/b\n",
            "    gpt2/h19/attn/compute_output_bias/o_b\n",
            "    gpt2/h19/norm_2/g\n",
            "    gpt2/h19/norm_2/b\n",
            "    gpt2/h19/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h20/norm_1/g\n",
            "    gpt2/h20/norm_1/b\n",
            "    gpt2/h20/attn/compute_output_bias/o_b\n",
            "    gpt2/h20/norm_2/g\n",
            "    gpt2/h20/norm_2/b\n",
            "    gpt2/h20/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h21/norm_1/g\n",
            "    gpt2/h21/norm_1/b\n",
            "Variable stacked/gpt2/h21/attn/compute_output_bias/o_b                size 36864        slice_size 18432        Shape[stacked=18, embd=2048]                                \n",
            "    gpt2/h21/attn/compute_output_bias/o_b\n",
            "    gpt2/h21/norm_2/g\n",
            "    gpt2/h21/norm_2/b\n",
            "    gpt2/h21/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h22/norm_1/g\n",
            "    gpt2/h22/norm_1/b\n",
            "    gpt2/h22/attn/compute_output_bias/o_b\n",
            "    gpt2/h22/norm_2/g\n",
            "    gpt2/h22/norm_2/b\n",
            "    gpt2/h22/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h23/norm_1/g\n",
            "    gpt2/h23/norm_1/b\n",
            "    gpt2/h23/attn/compute_output_bias/o_b\n",
            "    gpt2/h23/norm_2/g\n",
            "    gpt2/h23/norm_2/b\n",
            "    gpt2/h23/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/ln_f/g\n",
            "    gpt2/ln_f/b\n",
            "Trainable Variables            count: 150     Total size: 1315575808       Total slice_size: 204753920      \n",
            "All Variables                  count: 150     Total size: 1315575808       Total slice_size: 204753920      \n",
            "Counters:\n",
            "allreduce: 2.51e+10\n",
            " allreduce/[0]: 6.44e+09\n",
            "  allreduce/[0]/einsum_op: 6.44e+09\n",
            " allreduce/[1]: 1.87e+10\n",
            "  allreduce/[1]/einsum_op: 1.87e+10\n",
            "  allreduce/[1]/reduce_op: 2.54e+07\n",
            "einsum: 3.58e+13\n",
            "einsum_unique: 2.52e+13\n",
            "output: 3.14e+11\n",
            " output/AddOperation: 8.13e+10\n",
            " output/BinaryOpWithBroadcasting: 5.2e+08\n",
            " output/BroadcastOperation: 6.48e+09\n",
            " output/ConcatOperation: 3.22e+09\n",
            " output/Constant: 1.97e+05\n",
            " output/EinsumOperation: 7.64e+10\n",
            " output/ImportOperation: 2.62e+05\n",
            " output/OneHotOperation: 1.32e+10\n",
            " output/RangeOperation: 2.38e+05\n",
            " output/ReduceOperation: 3.85e+07\n",
            " output/ReshapeOperation: 1.21e+10\n",
            " output/ScalarAddOperation: 6.45e+09\n",
            " output/ScalarMultiplyOperation: 2.27e+10\n",
            " output/ShiftOperation: 1.61e+09\n",
            " output/SlicewiseOperation: 7.22e+10\n",
            " output/StackedVariable: 1.59e+06\n",
            " output/StopGradient: 1.63e+10\n",
            " output/TopKOperation: 2.62e+05\n",
            " output/UnstackOperation: 1.59e+06\n",
            " output/Variable: 1.64e+09\n",
            "output_unique: 1.51e+11\n",
            " output_unique/AddOperation: 3.88e+10\n",
            " output_unique/BinaryOpWithBroadcasting: 6.71e+07\n",
            " output_unique/BroadcastOperation: 6.45e+09\n",
            " output_unique/ConcatOperation: 1.61e+09\n",
            " output_unique/Constant: 2.46e+04\n",
            " output_unique/EinsumOperation: 3.15e+10\n",
            " output_unique/ImportOperation: 3.28e+04\n",
            " output_unique/OneHotOperation: 1.65e+09\n",
            " output_unique/RangeOperation: 3.07e+04\n",
            " output_unique/ReduceOperation: 1.43e+07\n",
            " output_unique/ReshapeOperation: 6.44e+09\n",
            " output_unique/ScalarAddOperation: 3.22e+09\n",
            " output_unique/ScalarMultiplyOperation: 1.05e+10\n",
            " output_unique/ShiftOperation: 8.05e+08\n",
            " output_unique/SlicewiseOperation: 4e+10\n",
            " output_unique/StackedVariable: 4.96e+05\n",
            " output_unique/StopGradient: 8.88e+09\n",
            " output_unique/TopKOperation: 3.28e+04\n",
            " output_unique/UnstackOperation: 4.96e+05\n",
            " output_unique/Variable: 1.32e+09\n",
            "variables: 1.32e+09\n",
            " variables/trainable: 1.32e+09\n",
            "From /content/GPTNeo/model_fns.py:235: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3423: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Done calling model_fn.\n",
            "Starting evaluation at 2021-03-28T01:03:17Z\n",
            "TPU job name worker\n",
            "Graph was finalized.\n",
            "Restoring parameters from gs://ditto_gptneo/GPT3_XL/model.ckpt-362000\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:824: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "Starting infeed thread controller.\n",
            "Starting outfeed thread controller.\n",
            "Initialized dataset iterators in 0 seconds\n",
            "Before copy master to slices.\n",
            "Done with copy master to slices.\n",
            "Enqueue next (26) batch(es) of data to infeed.\n",
            "Dequeue next (26) batch(es) of data from outfeed.\n",
            "Outfeed finished for iteration (0, 0)\n",
            "Evaluation [26/26]\n",
            "Stop infeed thread controller\n",
            "Shutting down InfeedController thread.\n",
            "InfeedController received shutdown signal, stopping.\n",
            "Infeed thread finished, shutting down.\n",
            "infeed marked as finished\n",
            "Stop output thread controller\n",
            "Shutting down OutfeedController thread.\n",
            "OutfeedController received shutdown signal, stopping.\n",
            "Outfeed thread finished, shutting down.\n",
            "outfeed marked as finished\n",
            "Shutdown TPU system.\n",
            "Inference Time : 32.88018s\n",
            "Finished evaluation at 2021-03-28-01:03:50\n",
            "Saving dict for global step 362000: global_step = 362000, lambada_acc = 0.6491364, lambada_log_ppl = 1.614934, loss = 12.647692\n",
            "Saving 'checkpoint_path' summary for global step 362000: gs://ditto_gptneo/GPT3_XL/model.ckpt-362000\n",
            "evaluation_loop marked as finished\n",
            "Eval task 'lambada' results: {'lambada_acc': 0.6491364, 'lambada_log_ppl': 1.614934, 'loss': 12.647692, 'global_step': 362000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jClDvH2bF66o"
      },
      "source": [
        "**Deployment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "tsDBK2cWKelQ",
        "outputId": "4bd5ef40-857d-4e62-bff7-9fa6f463eb82"
      },
      "source": [
        "!pip install urllib3==1.25.1\n",
        "#!pip3 install watson_machine_learning_client"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'urllib3' candidate (version 1.25.1 at https://files.pythonhosted.org/packages/c0/1f/516c14fd47ced1a2e2882edd776241c5b707ffc9051cd372843579829994/urllib3-1.25.1-py2.py3-none-any.whl#sha256=a9645efd62b9fc1c7cad8ed93e162aad4c6bfd90e143966ddd4099b78cd244be (from https://pypi.org/simple/urllib3/) (requires-python:>=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, <4))\n",
            "Reason for being yanked: Broken release\u001b[0m\n",
            "Collecting urllib3==1.25.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/1f/516c14fd47ced1a2e2882edd776241c5b707ffc9051cd372843579829994/urllib3-1.25.1-py2.py3-none-any.whl (150kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 13.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 20kB 10.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 30kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 40kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 51kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 61kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 71kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 81kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 92kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 102kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 112kB 6.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 122kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 133kB 6.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 143kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 6.2MB/s \n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: botocore 1.20.39 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3\n",
            "  Found existing installation: urllib3 1.26.4\n",
            "    Uninstalling urllib3-1.26.4:\n",
            "      Successfully uninstalled urllib3-1.26.4\n",
            "Successfully installed urllib3-1.25.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: watson_machine_learning_client in /usr/local/lib/python3.7/dist-packages (1.0.391)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from watson_machine_learning_client) (1.17.39)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from watson_machine_learning_client) (1.25.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from watson_machine_learning_client) (1.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from watson_machine_learning_client) (4.41.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from watson_machine_learning_client) (2020.12.5)\n",
            "Requirement already satisfied: lomond in /usr/local/lib/python3.7/dist-packages (from watson_machine_learning_client) (0.3.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from watson_machine_learning_client) (0.8.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from watson_machine_learning_client) (2.23.0)\n",
            "Requirement already satisfied: ibm-cos-sdk in /usr/local/lib/python3.7/dist-packages (from watson_machine_learning_client) (2.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from boto3->watson_machine_learning_client) (0.3.6)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.39 in /usr/local/lib/python3.7/dist-packages (from boto3->watson_machine_learning_client) (1.20.39)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->watson_machine_learning_client) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->watson_machine_learning_client) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->watson_machine_learning_client) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->watson_machine_learning_client) (2.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from lomond->watson_machine_learning_client) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->watson_machine_learning_client) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->watson_machine_learning_client) (2.10)\n",
            "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.10.0 in /usr/local/lib/python3.7/dist-packages (from ibm-cos-sdk->watson_machine_learning_client) (2.10.0)\n",
            "Requirement already satisfied: ibm-cos-sdk-core==2.10.0 in /usr/local/lib/python3.7/dist-packages (from ibm-cos-sdk->watson_machine_learning_client) (2.10.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owL5jVVM6asy"
      },
      "source": [
        "# import dependencies\n",
        "from ibm_watson_machine_learning import APIClient\n",
        "\n",
        "# Store access credentials\n",
        "watson_ml_credentials = {\n",
        "    \"apikey\": \"q-BsYkp-xwvModz7CVPWKuuIxVCm8MEeMXrJFnrXQl1E\",\n",
        "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
        "}\n",
        "\n",
        "wml_client = APIClient(watson_ml_credentials)\n",
        "\n",
        "# Specify model constants\n",
        "MODEL_NAME = \"GPT3_XL\"\n",
        "DEPLOYMENT_NAME = \"youtubePredictor_gptNeo\"\n",
        "BEST_MODEL = pretrained_model\n",
        "\n",
        "# Specify IBM Watson Machine Learning schema\n",
        "model_properties = {\n",
        "    wml_client.repository.ModelMetaNames.NAME: \"{}\".format(MODEL_NAME)\n",
        "}\n",
        "\n",
        "#Save Model\n",
        "# @TODO add training_data, i.e. x values, from init.csv\n",
        "# @TODO add training_target, i.e. y values from init.csv\n",
        "# @TODO add pipeline, i.e. \n",
        "published_model_details = wml_client.repository.store_model(model=BEST_MODEL, meta_props=model_properties,training_data=,training_target=, pipeline=,)\n",
        "model_uid = wml_client.repository.get_model_uid(model_details=published_model_details)\n",
        "deployment = wml_client.deployments.create(artifact_uid=model_uid, name=DEPLOYMENT_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjKmAk_dJXrp"
      },
      "source": [
        "!pip3 install git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "from transformers import GPT2Tokenizer, TFGPT2Model\n",
        "\n",
        "text_filename = \"prompt file absolute path goes here\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "evaluation_model = TFGPT2Model.from_pretrained('gpt2')\n",
        "generator = pipeline('question-answering', model=evaluation_model)\n",
        "set_seed(42)\n",
        " generator(\"insert instruction from prompt\", max_length=30, num_return_sequences=1)\n",
        "\n",
        "with open(text_filename) as f:\n",
        "    text = f.readlines()\n",
        " \n",
        "encoded_input = tokenizer(text, return_tensors='tf')\n",
        "output = evaluation_model(encoded_input)\n",
        "output.save(\"/content/GPTNeo/models/gpt2/trained_gpt2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VqStwdhIIhM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}