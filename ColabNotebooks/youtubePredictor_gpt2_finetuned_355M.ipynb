{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "youtubePredictor_gpt2_finetuned_355M.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "*GPT2 Training Notebook for DiTTo Youtube Predictor* \n",
        "\n",
        "Original by [Max Woolf](http://minimaxir.com) modified by [Greg Raiz](http://gregraiz.com) utilized by DiTTo team Stevens Institute of Technology SSW695A Spring2021\n",
        "\n",
        "For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple). \n",
        "Max Woolf blog on gpt2 [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use this notebook!\n",
        "Max Woolf original example notebook [GPT2 Notebook](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbXFMYXdidFZPRlZiWlFiX1JrR20zTXdKMXNvQXxBQ3Jtc0treE5ndS1JUzZoM0RzejZwTVFhbWlPUS0zWHBTbV90Snc3WlBhVDA1RmY4dFpCQWpqamtZendLS0xWTXJhdGdvTllfV2U0OGcwMTUxZ0QtY2NsTmJnN1hWbDRFS3g2M3phdFVvWklMLTdtM1BZcXlodw&q=http%3A%2F%2Fbit.ly%2Fgraiz_colab)\n",
        "Greg Raiz's [Youtube Tutorial](https://www.youtube.com/watch?v=R6KoIp1ETpM&t=247s)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9433ebbe-fe69-4efd-ea64-151d7e212bbe"
      },
      "source": [
        "model= \"355M\"  #@param ['124M', '355M', '774M', '1558M']\n",
        "# Note that these are millions of parameters.  \n",
        "# The 774M model is 3GB,\n",
        "# the 1558M model is 6GB. Start small, before going big.\n",
        "\n",
        "iterations =  226#@param {type: \"number\"}\n",
        "# If we're training, how many iterations do we want?\n",
        "\n",
        "trainingName = 'views_predictor' #@param {type: \"string\"}   \n",
        "# Each new model you train should be named. \n",
        "\n",
        "file_name = 'init.csv'  #@param {type: \"string\"}\n",
        "# If you have a training file in your Google drive, specify the filename\n",
        "# that will be used. \n",
        "\n",
        "\n",
        "%tensorflow_version 1.x        # This uses an older version of tensorflow\n",
        "!pip install -q gpt-2-simple   # You will get warnings but it's Ok. \n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.x        # This uses an older version of tensorflow`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c2d2cd-cd80-4c9e-a0fa-bd43ba7c4d3a"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Apr  6 15:34:32 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66fe0de9-9837-48e0-e28a-ce86f025dca7"
      },
      "source": [
        "# download selected model\n",
        "gpt2.download_gpt2(model_name=model)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 312Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 6.35Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 322Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [02:47, 8.46Mit/s]\n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 714Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 4.91Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 7.51Mit/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyJfH1bFHThf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba07fd66-bc07-4726-e78f-b875eab6b2a9"
      },
      "source": [
        "# mount google drive for model storage\n",
        "from google.colab import drive\n",
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c223af75-65c7-4ca9-d4f8-effeb44cdf32"
      },
      "source": [
        "# copy training file to notebook working directory\n",
        "gpt2.copy_file_from_gdrive(file_name) \n",
        "print(file_name)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf"
      },
      "source": [
        "# finetune model and save to google drive\n",
        "# parameters for gpt2.finetune:\n",
        "# restore_from: Set to fresh to start training from the base GPT-2, or set to latest to restart training from an existing checkpoint.\n",
        "# sample_every: Number of steps to print example output\n",
        "# print_every: Number of steps to print training progress\n",
        "# learning_rate: Learning rate for the training. (default 1e-4, can lower to 1e-5 if you have <1MB input data)\n",
        "# run_name: subfolder within checkpoint to save the model. This is useful if you want to work with multiple models (will also need to specify run_name when loading the model)\n",
        "# overwrite: Set to True if you want to continue finetuning an existing model (w/ restore_from='latest') without creating duplicate copies.\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,    # Filename, Model and Training Name are specified in the Init Function\n",
        "              model_name=model,\n",
        "              steps=iterations,\n",
        "              restore_from='latest',\n",
        "              overwrite=True,\n",
        "              run_name = trainingName,\n",
        "              print_every=10,\n",
        "              sample_every=10,\n",
        "              save_every=10,\n",
        "              )\n",
        "\n",
        "gpt2.copy_checkpoint_to_gdrive(run_name=trainingName)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "156e5ecc-3f31-45a5-b791-e0a5724ee6af"
      },
      "source": [
        "# copy trained model checkpoint to notebook working directory and load\n",
        "\n",
        "gpt2.copy_checkpoint_from_gdrive(run_name=trainingName)\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name=trainingName)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/views_predictor/model-226\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/views_predictor/model-226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29c1db32-5cec-4042-b07f-4294a716e37a"
      },
      "source": [
        "# Generate text from the trained model\n",
        "\n",
        "text_creativity = 70 #@param {type: \"slider\", min: 50, max: 100}\n",
        "# Changes how wacky the text gets. \n",
        "gpt2.generate(sess, run_name=trainingName,temperature=(text_creativity/100))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<|startoftext|>13<|endoftext|>\n",
            "<|startoftext|>14<|endoftext|>\n",
            "<|startoftext|>15<|endoftext|>\n",
            "<|startoftext|>16<|endoftext|>\n",
            "<|startoftext|>17<|endoftext|>\n",
            "<|startoftext|>18<|endoftext|>\n",
            "<|startoftext|>19<|endoftext|>\n",
            "<|startoftext|>20<|endoftext|>\n",
            "<|startoftext|>21<|endoftext|>\n",
            "<|startoftext|>22<|endoftext|>\n",
            "<|startoftext|>23<|endoftext|>\n",
            "<|startoftext|>24<|endoftext|>\n",
            "<|startoftext|>25<|endoftext|>\n",
            "<|startoftext|>26<|endoftext|>\n",
            "<|startoftext|>27<|endoftext|>\n",
            "<|startoftext|>28<|endoftext|>\n",
            "<|startoftext|>29<|endoftext|>\n",
            "<|startoftext|>30<|endoftext|>\n",
            "<|startoftext|>31<|endoftext|>\n",
            "<|startoftext|>32<|endoftext|>\n",
            "<|startoftext|>33<|endoftext|>\n",
            "<|startoftext|>34<|endoftext|>\n",
            "<|startoftext|>35<|endoftext|>\n",
            "<|startoftext|>36<|endoftext|>\n",
            "<|startoftext|>37<|endoftext|>\n",
            "<|startoftext|>38<|endoftext|>\n",
            "<|startoftext|>39<|endoftext|>\n",
            "<|startoftext|>40<|endoftext|>\n",
            "<|startoftext|>41<|endoftext|>\n",
            "<|startoftext|>42<|endoftext|>\n",
            "<|startoftext|>43<|endoftext|>\n",
            "<|startoftext|>44<|endoftext|>\n",
            "<|startoftext|>45<|endoftext|>\n",
            "<|startoftext|>46<|endoftext|>\n",
            "<|startoftext|>47<|endoftext|>\n",
            "<|startoftext|>48<|endoftext|>\n",
            "<|startoftext|>49<|endoftext|>\n",
            "<|startoftext|>50<|endoftext|>\n",
            "<|startoftext|>51<|endoftext|>\n",
            "<|startoftext|>52<|endoftext|>\n",
            "<|startoftext|>53<|endoftext|>\n",
            "<|startoftext|>54<|endoftext|>\n",
            "<|startoftext|>55<|endoftext|>\n",
            "<|startoftext|>56<|endoftext|>\n",
            "<|startoftext|>57<|endoftext|>\n",
            "<|startoftext|>58<|endoftext|>\n",
            "<|startoftext|>59<|endoftext|>\n",
            "<|startoftext|>60<|endoftext|>\n",
            "<|startoftext|>61<|endoftext|>\n",
            "<|startoftext|>62<|endoftext|>\n",
            "<|startoftext|>63<|endoftext|>\n",
            "<|startoftext|>64<|endoftext|>\n",
            "<|startoftext|>65<|endoftext|>\n",
            "<|startoftext|>66<|endoftext|>\n",
            "<|startoftext|>67<|endoftext|>\n",
            "<|startoftext|>68<|endoftext|>\n",
            "<|startoftext|>69<|endoftext|>\n",
            "<|startoftext|>70<|endoftext|>\n",
            "<|startoftext|>71<|endoftext|>\n",
            "<|startoftext|>72<|endoftext|>\n",
            "<|startoftext|>73<|endoftext|>\n",
            "<|startoftext|>74<|endoftext|>\n",
            "<|startoftext|>75<|endoftext|>\n",
            "<|startoftext|>76<|endoftext|>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e8a8b5-f51b-4d26-927d-cf29703c90f9"
      },
      "source": [
        "# parameters for gpt2.generate:\n",
        "# length: Number of tokens to generate (default 1023, the maximum)\n",
        "# temperature: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "# top_k: Limits the generated guesses to the top k guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set top_k=40)\n",
        "# top_p: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with top_p=0.9)\n",
        "# truncate: Truncates the input text until a given sequence, excluding that sequence (e.g. if truncate='<|endoftext|>', the returned text will include everything before the first <|endoftext|>). It may be useful to combine this with a smaller length if the input texts are short.\n",
        "# include_prefix: If using truncate and include_prefix=False, the specified prefix will not be included in the returned text.\n",
        "\n",
        "gpt2.generate(sess, \n",
        "              run_name=trainingName,\n",
        "              length=10,\n",
        "              prefix=\"Given [ANGER=0, DISGUST=0, FEAR=0, JOY=0.880435, SADNESS=0, TENTATIVE=0.8821536, ANALYTICAL=0.589295 , CONFIDENT=0.775702], Views=\",\n",
        "              nsamples=1,\n",
        "              batch_size=1\n",
        "              )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Given [ANGER=0, DISGUST=0, FEAR=0, JOY=0.880435, SADNESS=0, TENTATIVE=0.8821536, ANALYTICAL=0.589295 , CONFIDENT=0.775702], Views=<|endoftexttexttexttexttexttext\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0"
      },
      "source": [
        "# save generated text to a file\n",
        "\n",
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess, \n",
        "              run_name=trainingName,\n",
        "              destination_path=gen_file,\n",
        "              temperature=0.7,\n",
        "              length=10,\n",
        "              nsamples=1,\n",
        "              batch_size=1\n",
        "              )\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "29805dbc-bdea-494f-ae7e-02cda8cc62ed"
      },
      "source": [
        "# download file to local directory.  may have to run twice to get file to download\n",
        "\n",
        "files.download(gen_file)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_3b932f63-49b8-4d90-bab1-5c57f1933ec5\", \"gpt2_gentext_20210406_171148.txt\", 20)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7WwzLvTAOGbZ",
        "outputId": "a8304655-c467-4bd3-c7c9-66f52a05e684"
      },
      "source": [
        "# Save the model to local directory\n",
        "\n",
        "import pickle\n",
        "\n",
        "model_filename = 'youtubePredictor_gpt2_finetuned_355M.sav'\n",
        "pickle.dump(trainingName, open(model_filename, 'wb'))\n",
        "files.download(model_filename)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_1fd214f7-cc93-4c4c-97ea-49ffd0fa6cb2\", \"youtubePredictor_gpt2_finetuned_355M.sav\", 25)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}